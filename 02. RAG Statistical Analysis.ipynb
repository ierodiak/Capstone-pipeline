{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224c2df6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Import the enhanced modules\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatistical_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StatisticalAnalyzer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msystem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExperimentRunner\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_project_directories, verify_api_keys\n",
      "File \u001b[0;32m~/Documents/LSE/Courses/Capstone /Group Repos/RAG Pipeline/utils/statistical_analysis.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairwise_tukeyhsd\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manova\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m anova_lm\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformula\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ols\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the enhanced modules\n",
    "from utils.statistical_analysis import StatisticalAnalyzer\n",
    "from system.experiment_runner import ExperimentRunner\n",
    "from utils.helpers import create_project_directories, verify_api_keys\n",
    "\n",
    "# Set random seed for reproducibility (important for research)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize statistical analyzer\n",
    "stat_analyzer = StatisticalAnalyzer(alpha=0.05)  # 5% significance level\n",
    "\n",
    "print(\"Statistical analysis environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research questions and hypotheses\n",
    "research_questions = {\n",
    "    \"RQ1\": \"Does chunking method significantly affect retrieval quality?\",\n",
    "    \"RQ2\": \"Is there an interaction between embedding model and retrieval strategy?\",\n",
    "    \"RQ3\": \"What is the optimal trade-off between performance and response time?\"\n",
    "}\n",
    "\n",
    "# Define null hypotheses\n",
    "null_hypotheses = {\n",
    "    \"H0_1\": \"There is no significant difference in faithfulness scores between chunking methods\",\n",
    "    \"H0_2\": \"There is no interaction effect between embedding model and retrieval strategy\",\n",
    "    \"H0_3\": \"There is no correlation between retrieval quality and response time\"\n",
    "}\n",
    "\n",
    "# Calculate required sample size for adequate power\n",
    "from statsmodels.stats.power import TTestPower\n",
    "\n",
    "power_analysis = TTestPower()\n",
    "required_n = power_analysis.solve_power(\n",
    "    effect_size=0.5,  # Medium effect size\n",
    "    power=0.8,        # 80% power\n",
    "    alpha=0.05        # 5% significance level\n",
    ")\n",
    "\n",
    "print(f\"Required sample size per group: {int(np.ceil(required_n))}\")\n",
    "print(f\"Recommended: Test at least {int(np.ceil(required_n))} questions per configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the enhanced experiment runner\n",
    "from utils.statistical_analysis import enhance_experiment_runner\n",
    "StatisticalExperimentRunner = enhance_experiment_runner()\n",
    "\n",
    "# Initialize runner with statistical considerations\n",
    "stat_runner = StatisticalExperimentRunner(\n",
    "    base_path=\"./experiments\",\n",
    "    min_runs_per_config=30  # Minimum for statistical validity\n",
    ")\n",
    "\n",
    "# Define test questions with ground truth for evaluation\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"What is the main methodology described?\",\n",
    "        \"reference\": \"The main methodology involves retrieval-augmented generation.\",\n",
    "        \"question_type\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the system handle errors?\",\n",
    "        \"reference\": \"The system handles errors through exception handling and logging.\",\n",
    "        \"question_type\": \"technical\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the performance characteristics?\",\n",
    "        \"reference\": \"Performance depends on retrieval strategy and model size.\",\n",
    "        \"question_type\": \"analytical\"\n",
    "    },\n",
    "    # Add at least 30 questions for statistical validity and more for power analysis\n",
    "]\n",
    "\n",
    "# Define configurations for controlled experiment\n",
    "from core.experiment_config import ExperimentConfig\n",
    "\n",
    "# Control configuration (baseline)\n",
    "control_config = ExperimentConfig(\n",
    "    experiment_name=\"control\",\n",
    "    tags=[\"baseline\", \"control\"],\n",
    "    chunker=ChunkerConfig(method=\"recursive\", chunk_size=500),\n",
    "    embedding=EmbeddingConfig(model=\"text-embedding-3-small\"),\n",
    "    retrieval=RetrievalConfig(strategy=\"vector\", top_k=5)\n",
    ")\n",
    "\n",
    "# Treatment configurations (variations to test)\n",
    "treatment_configs = [\n",
    "    ExperimentConfig(\n",
    "        experiment_name=\"semantic_chunking\",\n",
    "        tags=[\"treatment\", \"semantic\"],\n",
    "        chunker=ChunkerConfig(method=\"semantic\", semantic_threshold=0.8),\n",
    "        embedding=EmbeddingConfig(model=\"text-embedding-3-small\"),\n",
    "        retrieval=RetrievalConfig(strategy=\"vector\", top_k=5)\n",
    "    ),\n",
    "    ExperimentConfig(\n",
    "        experiment_name=\"hybrid_retrieval\",\n",
    "        tags=[\"treatment\", \"hybrid\"],\n",
    "        chunker=ChunkerConfig(method=\"recursive\", chunk_size=500),\n",
    "        embedding=EmbeddingConfig(model=\"text-embedding-3-small\"),\n",
    "        retrieval=RetrievalConfig(strategy=\"hybrid\", top_k=5)\n",
    "    ),\n",
    "    ExperimentConfig(\n",
    "        experiment_name=\"large_embedding\",\n",
    "        tags=[\"treatment\", \"embedding\"],\n",
    "        chunker=ChunkerConfig(method=\"recursive\", chunk_size=500),\n",
    "        embedding=EmbeddingConfig(model=\"text-embedding-3-large\"),\n",
    "        retrieval=RetrievalConfig(strategy=\"vector\", top_k=5)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run controlled experiment\n",
    "all_configs = [control_config] + treatment_configs\n",
    "results_df = await stat_runner.run_statistical_experiment(\n",
    "    configs=all_configs,\n",
    "    test_questions=test_questions,\n",
    "    document_path=\"./documents\",\n",
    "    runs_per_config=3  # Multiple runs to account for variability\n",
    ")\n",
    "\n",
    "print(f\"Collected {len(results_df)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a792982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for analysis\n",
    "df_prepared = stat_analyzer.prepare_data(results_df)\n",
    "\n",
    "# Check assumptions for ANOVA\n",
    "chunker_groups = [\n",
    "    df_prepared[df_prepared['chunker'] == method]['ragas_faithfulness'].values\n",
    "    for method in df_prepared['chunker'].unique()\n",
    "]\n",
    "\n",
    "assumptions = stat_analyzer.check_assumptions(chunker_groups, \"ANOVA_chunking\")\n",
    "\n",
    "print(\"Assumption Checks:\")\n",
    "print(f\"Normality tests: {assumptions['normality']}\")\n",
    "print(f\"Homogeneity of variance: {assumptions['homogeneity']}\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Q-Q plot for normality\n",
    "for i, group in enumerate(chunker_groups):\n",
    "    stats.probplot(group, dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title(\"Q-Q Plot for Normality Check\")\n",
    "\n",
    "# Box plot for variance\n",
    "df_prepared.boxplot(column='ragas_faithfulness', by='chunker', ax=axes[1])\n",
    "axes[1].set_title(\"Distribution by Chunking Method\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If assumptions are violated, consider transformations or non-parametric tests\n",
    "if not all(test['normal'] for test in assumptions['normality'].values()):\n",
    "    print(\"\\nWARNING: Normality assumption violated. Consider:\")\n",
    "    print(\"1. Log transformation of the data\")\n",
    "    print(\"2. Non-parametric tests (Kruskal-Wallis)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test H0_1: Effect of chunking method on faithfulness\n",
    "print(\"=== Testing H0_1: Effect of Chunking Method ===\")\n",
    "anova_result = stat_analyzer.anova_analysis(\n",
    "    df_prepared, \n",
    "    factor='chunker', \n",
    "    metric='ragas_faithfulness',\n",
    "    include_tukey=True\n",
    ")\n",
    "\n",
    "print(f\"F-statistic: {anova_result['f_statistic']:.4f}\")\n",
    "print(f\"p-value: {anova_result['p_value']:.4f}\")\n",
    "print(f\"Effect size (η²): {anova_result['eta_squared']:.4f} ({anova_result['effect_size']})\")\n",
    "\n",
    "if anova_result['significant']:\n",
    "    print(\"\\nReject H0_1: Chunking method has a significant effect on faithfulness\")\n",
    "    print(\"\\nPost-hoc analysis (Tukey HSD):\")\n",
    "    for pair in anova_result['post_hoc']['significant_pairs']:\n",
    "        print(f\"  - Significant difference between {pair[0]} and {pair[1]}\")\n",
    "else:\n",
    "    print(\"\\nFail to reject H0_1: No significant effect of chunking method\")\n",
    "\n",
    "# Test H0_2: Interaction between embedding and retrieval\n",
    "print(\"\\n=== Testing H0_2: Interaction Effect ===\")\n",
    "factorial_result = stat_analyzer.factorial_anova(\n",
    "    df_prepared,\n",
    "    factors=['embedding', 'retrieval'],\n",
    "    metric='ragas_faithfulness'\n",
    ")\n",
    "\n",
    "print(\"Significant effects:\")\n",
    "for effect in factorial_result['significant_effects']:\n",
    "    print(f\"  - {effect}\")\n",
    "\n",
    "# Test H0_3: Correlation between quality and response time\n",
    "print(\"\\n=== Testing H0_3: Correlation Analysis ===\")\n",
    "corr_result = stat_analyzer.correlation_analysis(\n",
    "    df_prepared,\n",
    "    metrics=['ragas_faithfulness', 'ragas_answer_relevancy', 'response_time']\n",
    ")\n",
    "\n",
    "print(\"Significant correlations:\")\n",
    "for corr in corr_result['significant_correlations']:\n",
    "    print(f\"  - {corr['metric1']} vs {corr['metric2']}: \"\n",
    "          f\"r={corr['correlation']:.3f}, p={corr['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384da34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare control vs each treatment\n",
    "print(\"=== Control vs Treatment Comparisons ===\")\n",
    "\n",
    "control_data = df_prepared[df_prepared['experiment_name'] == 'control']\n",
    "\n",
    "for treatment_config in treatment_configs:\n",
    "    treatment_data = df_prepared[df_prepared['experiment_name'] == treatment_config.experiment_name]\n",
    "    \n",
    "    if len(treatment_data) > 0:\n",
    "        comparison = stat_analyzer.paired_comparison(\n",
    "            df_prepared,\n",
    "            config1='control',\n",
    "            config2=treatment_config.experiment_name,\n",
    "            metric='ragas_faithfulness'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nControl vs {treatment_config.experiment_name}:\")\n",
    "        print(f\"  Mean difference: {comparison['mean_diff']:.4f} \"\n",
    "              f\"(95% CI: {comparison['confidence_interval'][0]:.4f}, \"\n",
    "              f\"{comparison['confidence_interval'][1]:.4f})\")\n",
    "        print(f\"  Cohen's d: {comparison['cohens_d']:.3f} ({comparison['effect_size']})\")\n",
    "        print(f\"  p-value: {comparison['p_value']:.4f}\")\n",
    "        \n",
    "        if comparison['significant']:\n",
    "            print(f\"  ✓ Significant difference detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0498ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forest plot for effect sizes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "comparisons = []\n",
    "for treatment_config in treatment_configs:\n",
    "    comp = stat_analyzer.paired_comparison(\n",
    "        df_prepared,\n",
    "        config1='control',\n",
    "        config2=treatment_config.experiment_name,\n",
    "        metric='ragas_faithfulness'\n",
    "    )\n",
    "    comparisons.append({\n",
    "        'name': treatment_config.experiment_name,\n",
    "        'mean_diff': comp['mean_diff'],\n",
    "        'ci_low': comp['confidence_interval'][0],\n",
    "        'ci_high': comp['confidence_interval'][1],\n",
    "        'significant': comp['significant']\n",
    "    })\n",
    "\n",
    "# Plot\n",
    "y_pos = np.arange(len(comparisons))\n",
    "colors = ['red' if c['significant'] else 'gray' for c in comparisons]\n",
    "\n",
    "ax.scatter([c['mean_diff'] for c in comparisons], y_pos, \n",
    "          c=colors, s=100, zorder=3)\n",
    "\n",
    "for i, comp in enumerate(comparisons):\n",
    "    ax.plot([comp['ci_low'], comp['ci_high']], [i, i], \n",
    "           color=colors[i], linewidth=2, zorder=2)\n",
    "\n",
    "ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([c['name'] for c in comparisons])\n",
    "ax.set_xlabel('Mean Difference from Control (95% CI)')\n",
    "ax.set_title('Treatment Effects Compared to Control')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f45488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we had sufficient power\n",
    "print(\"=== Statistical Power Analysis ===\")\n",
    "\n",
    "for metric in ['ragas_faithfulness', 'ragas_answer_relevancy', 'response_time']:\n",
    "    if metric in df_prepared.columns:\n",
    "        power_result = stat_analyzer.sample_size_analysis(\n",
    "            df_prepared, \n",
    "            metric=metric,\n",
    "            effect_size=0.5,  # Medium effect\n",
    "            power=0.8         # 80% power\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Current sample size: {power_result['current_sample_size']}\")\n",
    "        print(f\"  Current power: {power_result['current_power']:.3f}\")\n",
    "        print(f\"  Required for 80% power: {power_result['required_sample_size']}\")\n",
    "        \n",
    "        if not power_result['sufficient_power']:\n",
    "            print(f\"  ⚠️ Insufficient power - need {power_result['required_sample_size'] - power_result['current_sample_size']} more samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeec4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive statistical report\n",
    "report_path = stat_analyzer.generate_statistical_report(\n",
    "    df_prepared,\n",
    "    output_path=\"./experiments/statistical_report.html\"\n",
    ")\n",
    "\n",
    "print(f\"Statistical report generated: {report_path}\")\n",
    "\n",
    "# Export for SPSS/R if needed\n",
    "spss_data = stat_analyzer.export_for_spss(\n",
    "    df_prepared,\n",
    "    output_path=\"./experiments/rag_data_for_spss.csv\"\n",
    ")\n",
    "\n",
    "print(\"Data exported for external statistical software\")\n",
    "\n",
    "# Create publication-ready table\n",
    "summary_table = df_prepared.groupby(['chunker', 'retrieval']).agg({\n",
    "    'ragas_faithfulness': ['mean', 'std', 'count'],\n",
    "    'response_time': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nSummary Table for Publication:\")\n",
    "print(summary_table.to_latex(\n",
    "    caption=\"RAG Pipeline Performance by Configuration\",\n",
    "    label=\"tab:rag_performance\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9348b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate interpretation based on statistical findings\n",
    "print(\"=== Statistical Findings Summary ===\")\n",
    "\n",
    "# Check main effects\n",
    "main_effects = []\n",
    "for factor in ['chunker', 'retrieval', 'embedding']:\n",
    "    anova = stat_analyzer.anova_analysis(df_prepared, factor, 'ragas_faithfulness')\n",
    "    if 'significant' in anova and anova['significant']:\n",
    "        main_effects.append({\n",
    "            'factor': factor,\n",
    "            'p_value': anova['p_value'],\n",
    "            'effect_size': anova['eta_squared']\n",
    "        })\n",
    "\n",
    "print(\"\\n1. MAIN EFFECTS:\")\n",
    "if main_effects:\n",
    "    for effect in main_effects:\n",
    "        print(f\"   - {effect['factor']}: p={effect['p_value']:.4f}, η²={effect['effect_size']:.3f}\")\n",
    "else:\n",
    "    print(\"   - No significant main effects found\")\n",
    "\n",
    "print(\"\\n2. PRACTICAL SIGNIFICANCE:\")\n",
    "print(\"   Consider both statistical significance (p-values) and effect sizes\")\n",
    "print(\"   Small effects may be statistically significant but not practically important\")\n",
    "\n",
    "print(\"\\n3. LIMITATIONS:\")\n",
    "print(\"   - Sample size limitations may affect power\")\n",
    "print(\"   - Multiple comparisons increase Type I error risk\")\n",
    "print(\"   - Results specific to test dataset and questions\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS:\")\n",
    "print(\"   - Replicate findings with larger sample sizes\")\n",
    "print(\"   - Test on diverse datasets\")\n",
    "print(\"   - Consider Bonferroni correction for multiple comparisons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
