{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG Modular Architecture Showcase\n",
    "=================================\n",
    "\n",
    "This notebook demonstrates how to use the modular RAG evaluation system for:\n",
    "- Building and configuring RAG pipelines\n",
    "- Using the flexible metrics system\n",
    "- Running systematic evaluations\n",
    "- Comparing different configurations\n",
    "- Analyzing results\n",
    "\n",
    "Architecture Overview:\n",
    "- Core: Configuration management and type definitions\n",
    "- Metrics: Pluggable metrics registry with RAGAS and custom metrics\n",
    "- Evaluation: Modular evaluator that works with registered metrics\n",
    "- Pipelines: Different pipeline patterns (linear, parallel)\n",
    "- System: Main RAG orchestrator\n",
    "\n",
    "Key Features:\n",
    "- üèóÔ∏è Factory Pattern: Consistent component creation\n",
    "- üîå Strategy Pattern: Swappable implementations  \n",
    "- üìä Metrics Registry: Add custom metrics without touching core code\n",
    "- ‚ö° Async First: High-performance async operations\n",
    "- üéØ Type Safe: Full type hints for better IDE support\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 1. SETUP AND ENVIRONMENT CONFIGURATION\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to handle async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import configuration and system components\n",
    "from core.config import (\n",
    "    ConfigManager, RAGConfig, LoaderConfig, ChunkerConfig,\n",
    "    EmbeddingConfig, StorageConfig, RetrievalConfig, \n",
    "    GenerationConfig, MetricsConfig\n",
    ")\n",
    "from system.rag_system import ModularRAGSystem\n",
    "from utils.helpers import create_project_directories, verify_api_keys\n",
    "\n",
    "# Import metrics components\n",
    "from core.metrics_registry import BaseMetric, MetricRequirements, MetricResult\n",
    "\n",
    "# Setup project directories\n",
    "create_project_directories()\n",
    "\n",
    "# Verify API keys\n",
    "api_keys = verify_api_keys()\n",
    "if not api_keys.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è Please set OPENAI_API_KEY in your .env file\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 2. BASIC CONFIGURATION\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive configuration for the RAG system\n",
    "config = RAGConfig(\n",
    "    # Experiment metadata\n",
    "    experiment_name=\"rag_showcase\",\n",
    "    tags=[\"showcase\", \"modular\", \"evaluation\"],\n",
    "    \n",
    "    # Document loading configuration\n",
    "    loader=LoaderConfig(\n",
    "        type=\"text\",                    # Options: \"text\", \"text_image\", \"none\"\n",
    "        pdf_extract_images=False,       # Whether to extract images from PDFs\n",
    "        supported_formats=[\"pdf\", \"txt\", \"docx\"]\n",
    "    ),\n",
    "    \n",
    "    # Chunking strategy configuration\n",
    "    chunker=ChunkerConfig(\n",
    "        method=\"recursive\",             # Options: \"recursive\", \"semantic\", \"sentence\", \"fixed\", \"sliding_window\"\n",
    "        chunk_size=500,                 # Characters per chunk\n",
    "        chunk_overlap=50,               # Overlap between chunks\n",
    "        semantic_threshold=0.8          # For semantic chunker only\n",
    "    ),\n",
    "    \n",
    "    # Embedding model configuration\n",
    "    embedding=EmbeddingConfig(\n",
    "        provider=\"openai\",              # Options: \"openai\", \"cohere\", \"huggingface\", \"sentence_transformers\"\n",
    "        model=\"text-embedding-3-small\", # Specific model to use\n",
    "        dimension=1536,                 # Embedding dimension\n",
    "        batch_size=100                  # Batch size for embedding\n",
    "    ),\n",
    "    \n",
    "    # Vector storage configuration\n",
    "    storage=StorageConfig(\n",
    "        type=\"faiss\",                   # Options: \"faiss\", \"chroma\", \"pinecone\", \"weaviate\", \"qdrant\"\n",
    "        persist=True,                   # Whether to save vector store to disk\n",
    "        metric=\"cosine\"                 # Distance metric for similarity\n",
    "    ),\n",
    "    \n",
    "    # Retrieval strategy configuration\n",
    "    retrieval=RetrievalConfig(\n",
    "        strategy=\"vector\",              # Options: \"vector\", \"bm25\", \"hybrid\", \"mmr\", \"rerank\"\n",
    "        top_k=5,                        # Number of documents to retrieve\n",
    "        search_type=\"similarity\",       # Type of search to perform\n",
    "        hybrid_weights=[0.7, 0.3],      # Weights for hybrid retrieval [vector, bm25]\n",
    "        mmr_lambda=0.5                  # For MMR diversity\n",
    "    ),\n",
    "    \n",
    "    # Generation model configuration\n",
    "    generation=GenerationConfig(\n",
    "        provider=\"openai\",              # Options: \"openai\", \"anthropic\", \"cohere\", \"huggingface\", \"ollama\"\n",
    "        model=\"gpt-4o-mini\",           # Specific model to use\n",
    "        temperature=0.0,                # 0.0 = deterministic, higher = more creative\n",
    "        max_tokens=1000,                # Maximum tokens in response\n",
    "        prompt_template=\"default\"       # Which prompt template to use\n",
    "    ),\n",
    "    \n",
    "    # Metrics configuration - using the new flexible system\n",
    "    metrics=MetricsConfig(\n",
    "        metric_names=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"],\n",
    "        metric_groups=[\"performance\"]   # Will include all performance metrics\n",
    "    ),\n",
    "    \n",
    "    # Pipeline configuration\n",
    "    pipeline_type=\"linear\"              # Options: \"linear\", \"parallel\", \"iterative\"\n",
    ")\n",
    "\n",
    "# Create configuration manager to handle the config\n",
    "config_manager = ConfigManager(config)\n",
    "print(f\"Configuration created with ID: {config.experiment_id}\")\n",
    "print(f\"Variant ID: {config.get_variant_id()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 3. INITIALIZE RAG SYSTEM\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the modular RAG system with our configuration\n",
    "rag_system = ModularRAGSystem(config_manager)\n",
    "rag_system.initialize_components()\n",
    "\n",
    "# Check what metrics are available in the system\n",
    "display(Markdown(\"### Available Metrics\"))\n",
    "\n",
    "# Metrics that work without reference answers\n",
    "no_ref_metrics = rag_system.metric_factory.registry.get_available_metrics(has_reference=False)\n",
    "print(f\"Without reference: {no_ref_metrics}\")\n",
    "\n",
    "# Metrics that require reference answers\n",
    "ref_metrics = rag_system.metric_factory.registry.get_available_metrics(has_reference=True)\n",
    "print(f\"With reference: {ref_metrics}\")\n",
    "\n",
    "# Get metric groups\n",
    "print(\"\\nMetric groups available:\")\n",
    "if hasattr(rag_system.metric_factory.registry, '_metric_groups'):\n",
    "    for group_name in rag_system.metric_factory.registry._metric_groups:\n",
    "        metrics_in_group = rag_system.metric_factory.registry.get_metrics_by_group(group_name)\n",
    "        print(f\"- {group_name}: {[m.name for m in metrics_in_group]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 4. DOCUMENT PROCESSING\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your documents\n",
    "DOCUMENT_PATH = \"./documents/sample.pdf\"  # Update this to your document path\n",
    "\n",
    "# Check if document exists\n",
    "if not Path(DOCUMENT_PATH).exists():\n",
    "    print(f\"‚ö†Ô∏è Document not found at {DOCUMENT_PATH}\")\n",
    "    print(\"Creating a sample document for demonstration...\")\n",
    "    # For demo purposes, create a simple text file\n",
    "    Path(\"./documents\").mkdir(exist_ok=True)\n",
    "    with open(\"./documents/sample.txt\", \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "        Retrieval-Augmented Generation (RAG) is a powerful technique that combines \n",
    "        information retrieval with language generation. The system works by first \n",
    "        retrieving relevant documents from a knowledge base, then using these documents \n",
    "        as context for generating accurate and informed responses.\n",
    "        \n",
    "        The modular architecture of our RAG system provides several key benefits:\n",
    "        1. Flexibility: Easy component swapping through configuration\n",
    "        2. Extensibility: Add new strategies without modifying core code\n",
    "        3. Testability: Each component can be tested independently\n",
    "        4. Performance: Optimized async operations throughout\n",
    "        5. Observability: Comprehensive logging and metrics\n",
    "        \n",
    "        Key components include:\n",
    "        - Document loader and processor\n",
    "        - Text chunking strategy\n",
    "        - Embedding model for vectorization\n",
    "        - Vector store for efficient retrieval\n",
    "        - Retrieval strategy\n",
    "        - Language model for generation\n",
    "        - Evaluation metrics\n",
    "        \"\"\")\n",
    "    DOCUMENT_PATH = \"./documents/sample.txt\"\n",
    "\n",
    "# Load and process documents\n",
    "chunks = rag_system.load_and_process_documents(DOCUMENT_PATH)\n",
    "\n",
    "# Create or load vector store\n",
    "# force_rebuild=True will recreate the vector store even if it exists\n",
    "vector_store = rag_system.create_or_load_vector_store(chunks, force_rebuild=True)\n",
    "\n",
    "print(f\"\\nüìä Processing Summary:\")\n",
    "print(f\"- Chunks created: {len(chunks)}\")\n",
    "print(f\"- Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} chars\")\n",
    "\n",
    "# Test the vector store with a sample query\n",
    "rag_system.vs_manager.test_retrieval(\"What is RAG?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 5. BUILDING RAG PIPELINES\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear pipeline (standard RAG flow)\n",
    "pipeline = rag_system.build_pipeline(pipeline_type=\"linear\")\n",
    "\n",
    "# Alternative: Build a parallel pipeline with multiple retrievers\n",
    "# This uses both vector and BM25 retrieval in parallel\n",
    "# pipeline = rag_system.build_pipeline(pipeline_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 6. BASIC QUERYING WITH EVALUATION\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_simple_query():\n",
    "    \"\"\"Demonstrate simple query without evaluation\"\"\"\n",
    "    \n",
    "    question = \"What is the main topic of the document?\"\n",
    "    \n",
    "    # Query without evaluation - just get the answer\n",
    "    result = await rag_system.query(\n",
    "        question=question,\n",
    "        evaluate=False  # No evaluation metrics\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚ùì Question: {result['question']}\")\n",
    "    print(f\"üí° Answer: {result['answer']}\")\n",
    "    print(f\"‚è±Ô∏è  Response time: {result['response_time']:.2f}s\")\n",
    "    print(f\"üìÑ Retrieved {result['num_contexts']} contexts\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the simple query\n",
    "simple_result = await run_simple_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 7. QUERY WITH AUTOMATIC EVALUATION (NO REFERENCE)\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluated_query():\n",
    "    \"\"\"Demonstrate query with evaluation metrics but no reference answer\"\"\"\n",
    "    \n",
    "    # Query with automatic evaluation using modular system\n",
    "    result = await rag_system.query(\n",
    "        question=\"What are the key components of the RAG system?\",\n",
    "        evaluate=True,                  # Enable evaluation\n",
    "        use_modular=True,               # Use new modular evaluation system\n",
    "        metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüí° Answer: {result['answer']}\")\n",
    "    \n",
    "    # Display evaluation results\n",
    "    if 'evaluation' in result:\n",
    "        print(\"\\nüìä Evaluation Results:\")\n",
    "        for metric_name, metric_result in result['evaluation'].items():\n",
    "            if metric_result['error']:\n",
    "                print(f\"‚ùå {metric_name}: Error - {metric_result['error']}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {metric_name}: {metric_result['value']:.3f} \"\n",
    "                      f\"(computed in {metric_result['computation_time']:.2f}s)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the evaluated query\n",
    "evaluated_result = await run_evaluated_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 8. EVALUATION WITH REFERENCE ANSWERS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_reference_evaluation():\n",
    "    \"\"\"Demonstrate evaluation with ground truth reference answers\"\"\"\n",
    "    \n",
    "    # Define test cases with reference answers\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is RAG?\",\n",
    "            \"reference\": \"RAG (Retrieval-Augmented Generation) is a technique that combines \"\n",
    "                        \"retrieval systems with language models to generate more accurate \"\n",
    "                        \"and contextual responses.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the benefits of modular architecture?\",\n",
    "            \"reference\": \"The modular architecture provides flexibility, extensibility, \"\n",
    "                        \"testability, performance optimization, and observability.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What metrics are available?\",\n",
    "            \"reference\": \"The system supports RAGAS metrics (faithfulness, answer relevancy, \"\n",
    "                        \"context precision/recall), custom metrics (response time, token \"\n",
    "                        \"efficiency, semantic similarity), and is extensible for new metrics.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run evaluation on all test cases\n",
    "    results = []\n",
    "    for test_case in test_cases:\n",
    "        print(f\"\\n‚ùì Evaluating: {test_case['question']}\")\n",
    "        \n",
    "        result = await rag_system.query(\n",
    "            question=test_case[\"question\"],\n",
    "            reference=test_case[\"reference\"],  # Provide reference for comparison\n",
    "            evaluate=True,\n",
    "            use_modular=True,\n",
    "            metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \n",
    "                    \"ragas_context_precision\", \"semantic_similarity\"]\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert results to DataFrame for analysis\n",
    "    eval_df = pd.DataFrame([\n",
    "        {\n",
    "            'question': r['question'],\n",
    "            'answer': r['answer'][:100] + '...',  # Truncate for display\n",
    "            **{k: v['value'] for k, v in r.get('evaluation', {}).items() \n",
    "               if not v.get('error')}\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüìä Evaluation Summary:\")\n",
    "    print(eval_df.to_string())\n",
    "    \n",
    "    return results, eval_df\n",
    "\n",
    "# Run reference evaluation\n",
    "ref_results, ref_df = await run_reference_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 9. USING METRIC GROUPS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_metric_groups():\n",
    "    \"\"\"Show how to use predefined metric groups\"\"\"\n",
    "    \n",
    "    print(\"\\nüìã Recommended Metric Sets:\")\n",
    "    \n",
    "    # Different use cases have different recommended metrics\n",
    "    use_cases = [\"general\", \"quality_focus\", \"performance_focus\", \"no_reference\"]\n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        metrics = rag_system.metric_factory.get_recommended_metrics(use_case)\n",
    "        print(f\"\\n{use_case}:\")\n",
    "        for metric in metrics:\n",
    "            print(f\"  - {metric}\")\n",
    "    \n",
    "    return use_cases\n",
    "\n",
    "# Show metric groups\n",
    "use_cases = demonstrate_metric_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 10. A/B TESTING DIFFERENT CONFIGURATIONS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_ab_testing():\n",
    "    \"\"\"Compare different RAG configurations\"\"\"\n",
    "    \n",
    "    # Create configuration variants to test\n",
    "    configs = {\n",
    "        \"small_chunks\": RAGConfig(\n",
    "            experiment_name=\"small_chunks\",\n",
    "            chunker=ChunkerConfig(\n",
    "                method=\"recursive\",\n",
    "                chunk_size=200,      # Smaller chunks\n",
    "                chunk_overlap=20\n",
    "            ),\n",
    "            retrieval=RetrievalConfig(\n",
    "                strategy=\"vector\",\n",
    "                top_k=7              # More chunks to compensate\n",
    "            ),\n",
    "            generation=GenerationConfig(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.0\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"large_chunks\": RAGConfig(\n",
    "            experiment_name=\"large_chunks\",\n",
    "            chunker=ChunkerConfig(\n",
    "                method=\"recursive\",\n",
    "                chunk_size=1000,     # Larger chunks\n",
    "                chunk_overlap=100\n",
    "            ),\n",
    "            retrieval=RetrievalConfig(\n",
    "                strategy=\"vector\",\n",
    "                top_k=3              # Fewer chunks needed\n",
    "            ),\n",
    "            generation=GenerationConfig(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.0\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"hybrid_retrieval\": RAGConfig(\n",
    "            experiment_name=\"hybrid_retrieval\",\n",
    "            chunker=ChunkerConfig(\n",
    "                method=\"recursive\",\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=50\n",
    "            ),\n",
    "            retrieval=RetrievalConfig(\n",
    "                strategy=\"hybrid\",    # Use both vector and BM25\n",
    "                top_k=5,\n",
    "                hybrid_weights=[0.7, 0.3]\n",
    "            ),\n",
    "            generation=GenerationConfig(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.0\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Test questions for comparison\n",
    "    test_questions = [\n",
    "        \"What is the main purpose of this system?\",\n",
    "        \"How does the modular architecture work?\",\n",
    "        \"What are the benefits of using this approach?\"\n",
    "    ]\n",
    "    \n",
    "    # Run experiments for each configuration\n",
    "    experiment_results = {}\n",
    "    \n",
    "    for config_name, config_variant in configs.items():\n",
    "        print(f\"\\nüß™ Testing configuration: {config_name}\")\n",
    "        \n",
    "        # Create new system with variant configuration\n",
    "        variant_manager = ConfigManager(config_variant)\n",
    "        variant_system = ModularRAGSystem(variant_manager)\n",
    "        variant_system.initialize_components()\n",
    "        \n",
    "        # Reuse existing vector store to save time\n",
    "        variant_system.vs_manager = rag_system.vs_manager\n",
    "        variant_system.retriever_factory.vector_store = vector_store\n",
    "        variant_system.retriever_factory.documents = chunks\n",
    "        \n",
    "        # Build pipeline with new configuration\n",
    "        variant_system.build_pipeline()\n",
    "        \n",
    "        # Run tests\n",
    "        variant_results = []\n",
    "        for question in test_questions:\n",
    "            result = await variant_system.query(\n",
    "                question=question,\n",
    "                evaluate=True,\n",
    "                metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"]\n",
    "            )\n",
    "            variant_results.append(result)\n",
    "            print(f\"  ‚úì Tested: {question[:50]}...\")\n",
    "        \n",
    "        experiment_results[config_name] = variant_results\n",
    "    \n",
    "    return experiment_results, test_questions\n",
    "\n",
    "# Run A/B testing\n",
    "experiment_results, test_questions = await run_ab_testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 11. RESULTS ANALYSIS AND VISUALIZATION\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experiment_results(experiment_results):\n",
    "    \"\"\"Analyze and visualize A/B test results\"\"\"\n",
    "    \n",
    "    # Prepare data for analysis\n",
    "    analysis_data = []\n",
    "    for config_name, results in experiment_results.items():\n",
    "        for result in results:\n",
    "            row = {\n",
    "                'config': config_name,\n",
    "                'question': result['question'],\n",
    "                'response_time': result['response_time'],\n",
    "                'num_contexts': result['num_contexts']\n",
    "            }\n",
    "            \n",
    "            # Add evaluation metrics\n",
    "            if 'evaluation' in result:\n",
    "                for metric_name, metric_data in result['evaluation'].items():\n",
    "                    if not metric_data.get('error'):\n",
    "                        row[metric_name] = metric_data['value']\n",
    "            \n",
    "            analysis_data.append(row)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nüìä Summary Statistics by Configuration:\")\n",
    "    summary = analysis_df.groupby('config').agg({\n",
    "        'ragas_faithfulness': 'mean',\n",
    "        'ragas_answer_relevancy': 'mean',\n",
    "        'response_time': 'mean',\n",
    "        'num_contexts': 'mean'\n",
    "    }).round(3)\n",
    "    print(summary)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Configuration Comparison', fontsize=16)\n",
    "    \n",
    "    # 1. Faithfulness comparison\n",
    "    sns.boxplot(data=analysis_df, x='config', y='ragas_faithfulness', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Faithfulness Scores')\n",
    "    axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45)\n",
    "    axes[0, 0].set_ylim(0, 1.1)\n",
    "    \n",
    "    # 2. Answer relevancy comparison\n",
    "    sns.boxplot(data=analysis_df, x='config', y='ragas_answer_relevancy', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Answer Relevancy Scores')\n",
    "    axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45)\n",
    "    axes[0, 1].set_ylim(0, 1.1)\n",
    "    \n",
    "    # 3. Response time comparison\n",
    "    sns.boxplot(data=analysis_df, x='config', y='response_time', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Response Time (seconds)')\n",
    "    axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 4. Create a summary comparison\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create a text summary\n",
    "    best_config = summary.idxmax()\n",
    "    worst_config = summary.idxmin()\n",
    "    \n",
    "    summary_text = \"Performance Summary:\\n\\n\"\n",
    "    summary_text += \"Best performers:\\n\"\n",
    "    for metric, config in best_config.items():\n",
    "        summary_text += f\"- {metric}: {config} ({summary.loc[config, metric]:.3f})\\n\"\n",
    "    \n",
    "    summary_text += \"\\nAreas for improvement:\\n\"\n",
    "    for metric, config in worst_config.items():\n",
    "        if metric != 'response_time':  # Lower is better for response time\n",
    "            summary_text += f\"- {metric}: {config} ({summary.loc[config, metric]:.3f})\\n\"\n",
    "    \n",
    "    ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, \n",
    "            fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analysis_df, summary\n",
    "\n",
    "# Analyze the experiment results\n",
    "analysis_df, summary = analyze_experiment_results(experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 12. CREATING CUSTOM METRICS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletenessMetric(BaseMetric):\n",
    "    \"\"\"\n",
    "    Custom metric to evaluate answer completeness.\n",
    "    \n",
    "    This metric checks:\n",
    "    1. Whether the answer addresses key question words\n",
    "    2. Whether the answer length is reasonable\n",
    "    3. Overall coverage of the question topic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"answer_completeness\",\n",
    "            requirements=MetricRequirements(\n",
    "                requires_reference=False,    # Doesn't need ground truth\n",
    "                requires_contexts=True,      # Uses retrieved contexts\n",
    "                requires_question=True,      # Needs the question\n",
    "                requires_answer=True         # Needs the generated answer\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    async def compute(self, **kwargs) -> MetricResult:\n",
    "        \"\"\"Compute the completeness score\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract inputs\n",
    "        question = kwargs['question']\n",
    "        answer = kwargs['answer']\n",
    "        contexts = kwargs['contexts']\n",
    "        \n",
    "        # Simple heuristic: check if answer addresses key question words\n",
    "        question_words = set(question.lower().split())\n",
    "        answer_words = set(answer.lower().split())\n",
    "        \n",
    "        # Remove common words that don't carry meaning\n",
    "        common_words = {\n",
    "            'the', 'a', 'an', 'is', 'are', 'was', 'were', \n",
    "            'what', 'how', 'why', 'when', 'where', 'who',\n",
    "            'of', 'to', 'for', 'with', 'in', 'on', 'at'\n",
    "        }\n",
    "        question_words -= common_words\n",
    "        \n",
    "        # Calculate coverage of question words in answer\n",
    "        covered_words = question_words.intersection(answer_words)\n",
    "        coverage = len(covered_words) / len(question_words) if question_words else 0\n",
    "        \n",
    "        # Check if answer length is reasonable (normalize to 50 words)\n",
    "        word_count = len(answer.split())\n",
    "        length_score = min(1.0, word_count / 50)\n",
    "        \n",
    "        # Check if answer uses information from contexts\n",
    "        context_text = ' '.join(contexts).lower()\n",
    "        context_words = set(context_text.split()) - common_words\n",
    "        answer_context_overlap = answer_words.intersection(context_words)\n",
    "        context_usage = len(answer_context_overlap) / len(answer_words) if answer_words else 0\n",
    "        \n",
    "        # Combined score with weights\n",
    "        completeness_score = (\n",
    "            coverage * 0.4 +        # 40% weight on addressing question\n",
    "            length_score * 0.3 +    # 30% weight on answer length\n",
    "            context_usage * 0.3     # 30% weight on using context\n",
    "        )\n",
    "        \n",
    "        return MetricResult(\n",
    "            metric_name=self.name,\n",
    "            value=completeness_score,\n",
    "            metadata={\n",
    "                \"question_coverage\": coverage,\n",
    "                \"length_score\": length_score,\n",
    "                \"context_usage\": context_usage,\n",
    "                \"answer_word_count\": word_count,\n",
    "                \"covered_keywords\": list(covered_words)[:5]  # Top 5 covered words\n",
    "            },\n",
    "            computation_time=time.time() - start_time\n",
    "        )\n",
    "\n",
    "# Register the custom metric\n",
    "completeness_metric = CompletenessMetric()\n",
    "rag_system.metric_factory.registry.register_metric(\n",
    "    completeness_metric,\n",
    "    groups=[\"custom\", \"quality\"]  # Add to custom and quality groups\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Registered custom metric: {completeness_metric.name}\")\n",
    "\n",
    "# Test the custom metric\n",
    "async def test_custom_metric():\n",
    "    \"\"\"Test our custom completeness metric\"\"\"\n",
    "    \n",
    "    result = await rag_system.query(\n",
    "        question=\"What are all the components of the RAG pipeline and how do they work together?\",\n",
    "        evaluate=True,\n",
    "        metrics=[\"answer_completeness\", \"ragas_faithfulness\", \"response_time\"]\n",
    "    )\n",
    "    \n",
    "    # Display custom metric results\n",
    "    if 'evaluation' in result and 'answer_completeness' in result['evaluation']:\n",
    "        completeness_data = result['evaluation']['answer_completeness']\n",
    "        \n",
    "        print(\"\\nüìä Answer Completeness Analysis:\")\n",
    "        print(f\"Overall Score: {completeness_data['value']:.3f}\")\n",
    "        print(\"\\nDetailed Breakdown:\")\n",
    "        for key, value in completeness_data['metadata'].items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run custom metric test\n",
    "custom_result = await test_custom_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 13. BATCH EVALUATION AND EXPORT\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_batch_evaluation():\n",
    "    \"\"\"Run evaluation on multiple questions and export results\"\"\"\n",
    "    \n",
    "    # Prepare a comprehensive test suite\n",
    "    test_suite = [\n",
    "        {\"question\": \"What is the purpose of the metrics registry?\", \"category\": \"architecture\"},\n",
    "        {\"question\": \"How do I add a new evaluation metric?\", \"category\": \"usage\"},\n",
    "        {\"question\": \"What are the different pipeline types available?\", \"category\": \"features\"},\n",
    "        {\"question\": \"How does the vector store work?\", \"category\": \"technical\"},\n",
    "        {\"question\": \"What configuration options are available?\", \"category\": \"configuration\"},\n",
    "        {\"question\": \"How can I optimize retrieval performance?\", \"category\": \"optimization\"},\n",
    "        {\"question\": \"What are the benefits of modular design?\", \"category\": \"architecture\"},\n",
    "        {\"question\": \"How do I export evaluation results?\", \"category\": \"usage\"}\n",
    "    ]\n",
    "    \n",
    "    # Run batch evaluation\n",
    "    batch_results = []\n",
    "    print(\"\\nüîÑ Running batch evaluation...\")\n",
    "    \n",
    "    for i, test in enumerate(test_suite):\n",
    "        print(f\"\\n[{i+1}/{len(test_suite)}] Evaluating: {test['question'][:50]}...\")\n",
    "        \n",
    "        result = await rag_system.query(\n",
    "            question=test[\"question\"],\n",
    "            evaluate=True,\n",
    "            metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \n",
    "                    \"answer_completeness\", \"response_time\"]\n",
    "        )\n",
    "        \n",
    "        # Add category to result\n",
    "        result['category'] = test['category']\n",
    "        batch_results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    batch_df = pd.DataFrame([\n",
    "        {\n",
    "            'question': r['question'],\n",
    "            'category': r['category'],\n",
    "            'answer_preview': r['answer'][:100] + '...',\n",
    "            **{k: v['value'] for k, v in r.get('evaluation', {}).items() \n",
    "               if not v.get('error')}\n",
    "        }\n",
    "        for r in batch_results\n",
    "    ])\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä Batch Evaluation Results:\")\n",
    "    print(batch_df.to_string())\n",
    "    \n",
    "    # Export results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"./results/rag_showcase_{timestamp}.csv\"\n",
    "    batch_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nüìÅ Results exported to: {output_file}\")\n",
    "    \n",
    "    # Also save detailed results as JSON\n",
    "    import json\n",
    "    json_file = f\"./results/rag_showcase_detailed_{timestamp}.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(batch_results, f, indent=2, default=str)\n",
    "    print(f\"üìÅ Detailed results saved to: {json_file}\")\n",
    "    \n",
    "    return batch_df, batch_results\n",
    "\n",
    "# Run batch evaluation\n",
    "batch_df, batch_results = await run_batch_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 14. SAVE CONFIGURATION AND FINAL SUMMARY\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save successful configuration for future use\n",
    "config_path = \"./configs/showcase_config.yaml\"\n",
    "config_manager.save_config(config_path)\n",
    "print(f\"\\n‚úÖ Configuration saved to: {config_path}\")\n",
    "\n",
    "# Create a final summary report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ RAG MODULAR ARCHITECTURE SHOWCASE - COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìã What we demonstrated:\")\n",
    "print(\"‚úì Simple configuration with smart defaults\")\n",
    "print(\"‚úì Component swapping through configuration\")\n",
    "print(\"‚úì Flexible metrics system with custom metrics\")\n",
    "print(\"‚úì Query evaluation with and without references\")\n",
    "print(\"‚úì A/B testing different configurations\")\n",
    "print(\"‚úì Results analysis and visualization\")\n",
    "print(\"‚úì Batch evaluation and export\")\n",
    "\n",
    "print(\"\\nüèÜ Key Architecture Benefits:\")\n",
    "print(\"1. Factory Pattern:\")\n",
    "print(\"   - Consistent component creation\")\n",
    "print(\"   - Easy to add new strategies\")\n",
    "print(\"   - No code changes needed\")\n",
    "\n",
    "print(\"\\n2. Strategy Pattern:\")\n",
    "print(\"   - Swappable implementations\")\n",
    "print(\"   - Clean interfaces\")\n",
    "print(\"   - Independent testing\")\n",
    "\n",
    "print(\"\\n3. Metrics Registry:\")\n",
    "print(\"   - Pluggable metrics\")\n",
    "print(\"   - Custom metrics without core changes\")\n",
    "print(\"   - Organized by groups\")\n",
    "\n",
    "print(\"\\n4. Async Operations:\")\n",
    "print(\"   - High performance\")\n",
    "print(\"   - Concurrent processing\")\n",
    "print(\"   - Non-blocking evaluation\")\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"1. Try different configurations\")\n",
    "print(\"2. Add your own custom metrics\")\n",
    "print(\"3. Extend with new strategies\")\n",
    "print(\"4. Run large-scale experiments\")\n",
    "print(\"5. Deploy to production\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Happy experimenting with the modular RAG system! üöÄ\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
