{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf75909e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LoaderConfig' from 'core.config' (/Users/ihebbz/Documents/LSE/Courses/Capstone /Group Repos/RAG Pipeline/core/config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Import all necessary components from our RAG system\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     ConfigManager, RAGConfig, LoaderConfig, ChunkerConfig,\n\u001b[1;32m     13\u001b[0m     EmbeddingConfig, StorageConfig, RetrievalConfig, \n\u001b[1;32m     14\u001b[0m     GenerationConfig, MetricsConfig\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msystem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModularRAGSystem\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_project_directories, verify_api_keys\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LoaderConfig' from 'core.config' (/Users/ihebbz/Documents/LSE/Courses/Capstone /Group Repos/RAG Pipeline/core/config.py)"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "# Apply nest_asyncio to handle async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Import all necessary components from our RAG system\n",
    "from core.config import (\n",
    "    ConfigManager, RAGConfig, LoaderConfig, ChunkerConfig,\n",
    "    EmbeddingConfig, StorageConfig, RetrievalConfig, \n",
    "    GenerationConfig, MetricsConfig\n",
    ")\n",
    "from system.rag_system import ModularRAGSystem\n",
    "from utils.helpers import create_project_directories, verify_api_keys\n",
    "from core.metrics_registry import BaseMetric, MetricRequirements, MetricResult\n",
    "\n",
    "# Setup project directories (documents/, data/, configs/, results/, logs/)\n",
    "create_project_directories()\n",
    "\n",
    "# Verify that required API keys are set in environment\n",
    "api_keys = verify_api_keys()\n",
    "if not api_keys.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"\u26a0\ufe0f Please set OPENAI_API_KEY in your .env file\")\n",
    "else:\n",
    "    print(\"\u2705 Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. BASIC CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Create a comprehensive configuration for the RAG system\n",
    "config = RAGConfig(\n",
    "    # Experiment metadata\n",
    "    experiment_name=\"rag_evaluation_demo\",\n",
    "    tags=[\"demo\", \"modular\", \"evaluation\"],\n",
    "    \n",
    "    # Document loading configuration\n",
    "    loader=LoaderConfig(\n",
    "        type=\"text\",                    # Options: \"text\", \"text_image\", \"none\"\n",
    "        pdf_extract_images=False,       # Whether to extract images from PDFs\n",
    "        supported_formats=[\"pdf\", \"txt\", \"docx\"]\n",
    "    ),\n",
    "    \n",
    "    # Chunking strategy configuration\n",
    "    chunker=ChunkerConfig(\n",
    "        method=\"recursive\",             # Options: \"recursive\", \"semantic\", \"sentence\", \"fixed\", \"sliding_window\"\n",
    "        chunk_size=500,                 # Characters per chunk\n",
    "        chunk_overlap=50,               # Overlap between chunks\n",
    "        semantic_threshold=0.8          # For semantic chunker only\n",
    "    ),\n",
    "    \n",
    "    # Embedding model configuration\n",
    "    embedding=EmbeddingConfig(\n",
    "        provider=\"openai\",              # Options: \"openai\", \"cohere\", \"huggingface\", \"sentence_transformers\"\n",
    "        model=\"text-embedding-3-small\", # Specific model to use\n",
    "        dimension=1536,                 # Embedding dimension\n",
    "        batch_size=100                  # Batch size for embedding\n",
    "    ),\n",
    "    \n",
    "    # Vector storage configuration\n",
    "    storage=StorageConfig(\n",
    "        type=\"faiss\",                   # Options: \"faiss\", \"chroma\", \"pinecone\", \"weaviate\", \"qdrant\"\n",
    "        persist=True,                   # Whether to save vector store to disk\n",
    "        metric=\"cosine\"                 # Distance metric for similarity\n",
    "    ),\n",
    "    \n",
    "    # Retrieval strategy configuration\n",
    "    retrieval=RetrievalConfig(\n",
    "        strategy=\"vector\",              # Options: \"vector\", \"bm25\", \"hybrid\", \"mmr\", \"rerank\"\n",
    "        top_k=5,                        # Number of documents to retrieve\n",
    "        search_type=\"similarity\",       # Type of search to perform\n",
    "        hybrid_weights=[0.7, 0.3],      # Weights for hybrid retrieval [vector, bm25]\n",
    "        mmr_lambda=0.5                  # For MMR diversity\n",
    "    ),\n",
    "    \n",
    "    # Generation model configuration\n",
    "    generation=GenerationConfig(\n",
    "        provider=\"openai\",              # Options: \"openai\", \"anthropic\", \"cohere\", \"huggingface\", \"ollama\"\n",
    "        model=\"gpt-4o-mini\",           # Specific model to use\n",
    "        temperature=0.0,                # 0.0 = deterministic, higher = more creative\n",
    "        max_tokens=1000,                # Maximum tokens in response\n",
    "        prompt_template=\"default\"       # Which prompt template to use\n",
    "    ),\n",
    "    \n",
    "    # Metrics configuration - using the new flexible system\n",
    "    metrics=MetricsConfig(\n",
    "        metric_names=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"],\n",
    "        metric_groups=[\"performance\"]   # Will include all performance metrics\n",
    "    ),\n",
    "    \n",
    "    # Pipeline configuration\n",
    "    pipeline_type=\"linear\"              # Options: \"linear\", \"parallel\", \"iterative\"\n",
    ")\n",
    "\n",
    "# Create configuration manager to handle the config\n",
    "config_manager = ConfigManager()\n",
    "config_manager.config = config\n",
    "print(f\"Configuration created with ID: {config.experiment_id}\")\n",
    "print(f\"Variant ID: {config.get_variant_id()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. INITIALIZE RAG SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize the modular RAG system with our configuration\n",
    "rag_system = ModularRAGSystem(config_manager)\n",
    "rag_system.initialize_components()\n",
    "\n",
    "# Check what metrics are available in the system\n",
    "print(\"\\n\ud83d\udcca Available Metrics:\")\n",
    "\n",
    "# Metrics that work without reference answers\n",
    "no_ref_metrics = rag_system.metric_factory.registry.get_available_metrics(has_reference=False)\n",
    "print(f\"Without reference: {no_ref_metrics}\")\n",
    "\n",
    "# Metrics that require reference answers\n",
    "ref_metrics = rag_system.metric_factory.registry.get_available_metrics(has_reference=True)\n",
    "print(f\"With reference: {ref_metrics}\")\n",
    "\n",
    "# Get metric groups\n",
    "print(\"\\nMetric groups available:\")\n",
    "for group_name in rag_system.metric_factory.registry._metric_groups:\n",
    "    metrics_in_group = rag_system.metric_factory.registry.get_metrics_by_group(group_name)\n",
    "    print(f\"- {group_name}: {[m.name for m in metrics_in_group]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. DOCUMENT PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "# Specify the path to your documents\n",
    "DOCUMENT_PATH = \"./documents/sample.pdf\"  # Update this to your document path\n",
    "\n",
    "# Check if document exists\n",
    "if not Path(DOCUMENT_PATH).exists():\n",
    "    print(f\"\u26a0\ufe0f Document not found at {DOCUMENT_PATH}\")\n",
    "    print(\"Please create a 'documents' folder and add PDF files\")\n",
    "    # For demo purposes, you can create a simple text file\n",
    "    Path(\"./documents\").mkdir(exist_ok=True)\n",
    "    with open(\"./documents/sample.txt\", \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "        Retrieval-Augmented Generation (RAG) is a powerful technique that combines \n",
    "        information retrieval with language generation. The system works by first \n",
    "        retrieving relevant documents from a knowledge base, then using these documents \n",
    "        as context for generating accurate and informed responses.\n",
    "        \n",
    "        The key components of a RAG system include:\n",
    "        1. Document loader and processor\n",
    "        2. Text chunking strategy\n",
    "        3. Embedding model for vectorization\n",
    "        4. Vector store for efficient retrieval\n",
    "        5. Retrieval strategy\n",
    "        6. Language model for generation\n",
    "        7. Evaluation metrics\n",
    "        \"\"\")\n",
    "    DOCUMENT_PATH = \"./documents/sample.txt\"\n",
    "\n",
    "# Load and process documents\n",
    "chunks = rag_system.load_and_process_documents(DOCUMENT_PATH)\n",
    "\n",
    "# Create or load vector store\n",
    "# force_rebuild=True will recreate the vector store even if it exists\n",
    "vector_store = rag_system.create_or_load_vector_store(chunks, force_rebuild=True)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Processing Summary:\")\n",
    "print(f\"- Chunks created: {len(chunks)}\")\n",
    "print(f\"- Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} chars\")\n",
    "\n",
    "# Test the vector store with a sample query\n",
    "rag_system.vs_manager.test_retrieval(\"What is RAG?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. BUILDING RAG PIPELINES\n",
    "# ============================================================================\n",
    "\n",
    "# Build a linear pipeline (standard RAG flow)\n",
    "pipeline = rag_system.build_pipeline(pipeline_type=\"linear\")\n",
    "\n",
    "# Alternative: Build a parallel pipeline with multiple retrievers\n",
    "# This uses both vector and BM25 retrieval in parallel\n",
    "# pipeline = rag_system.build_pipeline(pipeline_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. BASIC QUERYING WITH EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "async def run_simple_query():\n",
    "    \"\"\"Demonstrate simple query without evaluation\"\"\"\n",
    "    \n",
    "    question = \"What is the main topic of the document?\"\n",
    "    \n",
    "    # Query without evaluation - just get the answer\n",
    "    result = await rag_system.query(\n",
    "        question=question,\n",
    "        evaluate=False  # No evaluation metrics\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\u2753 Question: {result['question']}\")\n",
    "    print(f\"\ud83d\udca1 Answer: {result['answer']}\")\n",
    "    print(f\"\u23f1\ufe0f  Response time: {result['response_time']:.2f}s\")\n",
    "    print(f\"\ud83d\udcc4 Retrieved {result['num_contexts']} contexts\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the simple query\n",
    "simple_result = await run_simple_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. QUERY WITH AUTOMATIC EVALUATION (NO REFERENCE)\n",
    "# ============================================================================\n",
    "\n",
    "async def run_evaluated_query():\n",
    "    \"\"\"Demonstrate query with evaluation metrics but no reference answer\"\"\"\n",
    "    \n",
    "    # Query with automatic evaluation using modular system\n",
    "    result = await rag_system.query(\n",
    "        question=\"What are the key components of the RAG system?\",\n",
    "        evaluate=True,                  # Enable evaluation\n",
    "        use_modular=True,               # Use new modular evaluation system\n",
    "        metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\ud83d\udca1 Answer: {result['answer']}\")\n",
    "    \n",
    "    # Display evaluation results\n",
    "    if 'evaluation' in result:\n",
    "        print(\"\\n\ud83d\udcca Evaluation Results:\")\n",
    "        for metric_name, metric_result in result['evaluation'].items():\n",
    "            if metric_result['error']:\n",
    "                print(f\"\u274c {metric_name}: Error - {metric_result['error']}\")\n",
    "            else:\n",
    "                print(f\"\u2705 {metric_name}: {metric_result['value']:.3f} \"\n",
    "                      f\"(computed in {metric_result['computation_time']:.2f}s)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the evaluated query\n",
    "evaluated_result = await run_evaluated_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. EVALUATION WITH REFERENCE ANSWERS\n",
    "# ============================================================================\n",
    "\n",
    "async def run_reference_evaluation():\n",
    "    \"\"\"Demonstrate evaluation with ground truth reference answers\"\"\"\n",
    "    \n",
    "    # Define test cases with reference answers\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is RAG?\",\n",
    "            \"reference\": \"RAG (Retrieval-Augmented Generation) is a technique that combines \"\n",
    "                        \"retrieval systems with language models to generate more accurate \"\n",
    "                        \"and contextual responses.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How does the evaluation system work?\",\n",
    "            \"reference\": \"The evaluation system uses a modular architecture with pluggable \"\n",
    "                        \"metrics that can assess different aspects of RAG performance \"\n",
    "                        \"including faithfulness, relevancy, and efficiency.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What metrics are available?\",\n",
    "            \"reference\": \"The system supports RAGAS metrics (faithfulness, answer relevancy, \"\n",
    "                        \"context precision/recall), custom metrics (response time, token \"\n",
    "                        \"efficiency, semantic similarity), and is extensible for new metrics.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run evaluation on all test cases\n",
    "    results = []\n",
    "    for test_case in test_cases:\n",
    "        print(f\"\\n\u2753 Evaluating: {test_case['question']}\")\n",
    "        \n",
    "        result = await rag_system.query(\n",
    "            question=test_case[\"question\"],\n",
    "            reference=test_case[\"reference\"],  # Provide reference for comparison\n",
    "            evaluate=True,\n",
    "            use_modular=True,\n",
    "            metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \n",
    "                    \"ragas_context_precision\", \"semantic_similarity\"]\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert results to DataFrame for analysis\n",
    "    eval_df = pd.DataFrame([\n",
    "        {\n",
    "            'question': r['question'],\n",
    "            'answer': r['answer'][:100] + '...',  # Truncate for display\n",
    "            **{k: v['value'] for k, v in r.get('evaluation', {}).items() \n",
    "               if not v.get('error')}\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Evaluation Summary:\")\n",
    "    print(eval_df.to_string())\n",
    "    \n",
    "    return results, eval_df\n",
    "\n",
    "# Run reference evaluation\n",
    "ref_results, ref_df = await run_reference_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92760095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. USING METRIC GROUPS\n",
    "# ============================================================================\n",
    "\n",
    "def demonstrate_metric_groups():\n",
    "    \"\"\"Show how to use predefined metric groups\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udccb Recommended Metric Sets:\")\n",
    "    \n",
    "    # Different use cases have different recommended metrics\n",
    "    use_cases = [\"general\", \"quality_focus\", \"performance_focus\", \"no_reference\"]\n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        metrics = rag_system.metric_factory.get_recommended_metrics(use_case)\n",
    "        print(f\"\\n{use_case}:\")\n",
    "        for metric in metrics:\n",
    "            print(f\"  - {metric}\")\n",
    "    \n",
    "    return use_cases\n",
    "\n",
    "# Show metric groups\n",
    "use_cases = demonstrate_metric_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG Evaluation Pipeline - Complete Guide\n",
    "========================================\n",
    "\n",
    "This script demonstrates how to use the modular RAG evaluation system for:\n",
    "- Building and configuring RAG pipelines\n",
    "- Using the flexible metrics system\n",
    "- Running systematic evaluations\n",
    "- Comparing different configurations\n",
    "- Analyzing results\n",
    "\n",
    "Architecture Overview:\n",
    "- Core: Configuration management and type definitions\n",
    "- Metrics: Pluggable metrics registry with RAGAS and custom metrics\n",
    "- Evaluation: Modular evaluator that works with registered metrics\n",
    "- Pipelines: Different pipeline patterns (linear, parallel)\n",
    "- System: Main RAG orchestrator\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SETUP AND ENVIRONMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Import all necessary components from our RAG system\n",
    "from core.config import (\n",
    "    ConfigManager, RAGConfig, LoaderConfig, ChunkerConfig,\n",
    "    EmbeddingConfig, StorageConfig, RetrievalConfig, \n",
    "    GenerationConfig, MetricsConfig\n",
    ")\n",
    "from system.rag_system import ModularRAGSystem\n",
    "from utils.helpers import create_project_directories, verify_api_keys\n",
    "from utils.statistical_analysis import StatisticalAnalyzer\n",
    "from core.metrics_registry import BaseMetric, MetricRequirements, MetricResult\n",
    "\n",
    "# Setup project directories (documents/, data/, configs/, results/, logs/)\n",
    "create_project_directories()\n",
    "\n",
    "# Verify that required API keys are set in environment\n",
    "api_keys = verify_api_keys()\n",
    "if not api_keys.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"\u26a0\ufe0f Please set OPENAI_API_KEY in your .env file\")\n",
    "else:\n",
    "    print(\"\u2705 Environment configured successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. BASIC CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Create a comprehensive configuration for the RAG system\n",
    "config = RAGConfig(\n",
    "    # Experiment metadata\n",
    "    experiment_name=\"rag_evaluation_demo\",\n",
    "    tags=[\"demo\", \"modular\", \"evaluation\"],\n",
    "    \n",
    "    # Document loading configuration\n",
    "    loader=LoaderConfig(\n",
    "        type=\"text\",                    # Options: \"text\", \"text_image\", \"none\"\n",
    "        pdf_extract_images=False,       # Whether to extract images from PDFs\n",
    "        supported_formats=[\"pdf\", \"txt\", \"docx\"]\n",
    "    ),\n",
    "    \n",
    "    # Chunking strategy configuration\n",
    "    chunker=ChunkerConfig(\n",
    "        method=\"recursive\",             # Options: \"recursive\", \"semantic\", \"sentence\", \"fixed\", \"sliding_window\"\n",
    "        chunk_size=500,                 # Characters per chunk\n",
    "        chunk_overlap=50,               # Overlap between chunks\n",
    "        semantic_threshold=0.8          # For semantic chunker only\n",
    "    ),\n",
    "    \n",
    "    # Embedding model configuration\n",
    "    embedding=EmbeddingConfig(\n",
    "        provider=\"openai\",              # Options: \"openai\", \"cohere\", \"huggingface\", \"sentence_transformers\"\n",
    "        model=\"text-embedding-3-small\", # Specific model to use\n",
    "        dimension=1536,                 # Embedding dimension\n",
    "        batch_size=100                  # Batch size for embedding\n",
    "    ),\n",
    "    \n",
    "    # Vector storage configuration\n",
    "    storage=StorageConfig(\n",
    "        type=\"faiss\",                   # Options: \"faiss\", \"chroma\", \"pinecone\", \"weaviate\", \"qdrant\"\n",
    "        persist=True,                   # Whether to save vector store to disk\n",
    "        metric=\"cosine\"                 # Distance metric for similarity\n",
    "    ),\n",
    "    \n",
    "    # Retrieval strategy configuration\n",
    "    retrieval=RetrievalConfig(\n",
    "        strategy=\"vector\",              # Options: \"vector\", \"bm25\", \"hybrid\", \"mmr\", \"rerank\"\n",
    "        top_k=5,                        # Number of documents to retrieve\n",
    "        search_type=\"similarity\",       # Type of search to perform\n",
    "        hybrid_weights=[0.7, 0.3],      # Weights for hybrid retrieval [vector, bm25]\n",
    "        mmr_lambda=0.5                  # For MMR diversity\n",
    "    ),\n",
    "    \n",
    "    # Generation model configuration\n",
    "    generation=GenerationConfig(\n",
    "        provider=\"openai\",              # Options: \"openai\", \"anthropic\", \"cohere\", \"huggingface\", \"ollama\"\n",
    "        model=\"gpt-4o-mini\",           # Specific model to use\n",
    "        temperature=0.0,                # 0.0 = deterministic, higher = more creative\n",
    "        max_tokens=1000,                # Maximum tokens in response\n",
    "        prompt_template=\"default\"       # Which prompt template to use\n",
    "    ),\n",
    "    \n",
    "    # Metrics configuration - using the new flexible system\n",
    "    metrics=MetricsConfig(\n",
    "        metric_names=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"],\n",
    "        metric_groups=[\"performance\"]   # Will include all performance metrics\n",
    "    ),\n",
    "    \n",
    "    # Pipeline configuration\n",
    "    pipeline_type=\"linear\"              # Options: \"linear\", \"parallel\", \"iterative\"\n",
    ")\n",
    "\n",
    "# Create configuration manager to handle the config\n",
    "config_manager = ConfigManager()\n",
    "config_manager.config = config\n",
    "print(f\"Configuration created with ID: {config.experiment_id}\")\n",
    "print(f\"Variant ID: {config.get_variant_id()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. INITIALIZE RAG SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize the modular RAG system with our configuration\n",
    "rag_system = ModularRAGSystem(config_manager)\n",
    "rag_system.initialize_components()\n",
    "\n",
    "# Check what metrics are available in the system\n",
    "print(\"\\n\ud83d\udcca Available Metrics:\")\n",
    "\n",
    "# Metrics that work without reference answers\n",
    "no_ref_metrics = rag_system.metric_factory.registry.get_available_metrics(has_reference=False)\n",
    "print(f\"Without reference: {no_ref_metrics}\")\n",
    "\n",
    "# Metrics that require reference answers\n",
    "ref_metrics = rag_system.metric_factory.registry.get_available_metrics(has_reference=True)\n",
    "print(f\"With reference: {ref_metrics}\")\n",
    "\n",
    "# Get metric groups\n",
    "print(\"\\nMetric groups available:\")\n",
    "for group_name in rag_system.metric_factory.registry._metric_groups:\n",
    "    metrics_in_group = rag_system.metric_factory.registry.get_metrics_by_group(group_name)\n",
    "    print(f\"- {group_name}: {[m.name for m in metrics_in_group]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DOCUMENT PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "# Specify the path to your documents\n",
    "DOCUMENT_PATH = \"./documents/sample.pdf\"  # Update this to your document path\n",
    "\n",
    "# Check if document exists\n",
    "if not Path(DOCUMENT_PATH).exists():\n",
    "    print(f\"\u26a0\ufe0f Document not found at {DOCUMENT_PATH}\")\n",
    "    print(\"Please create a 'documents' folder and add PDF files\")\n",
    "    # For demo purposes, you can create a simple text file\n",
    "    Path(\"./documents\").mkdir(exist_ok=True)\n",
    "    with open(\"./documents/sample.txt\", \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "        Retrieval-Augmented Generation (RAG) is a powerful technique that combines \n",
    "        information retrieval with language generation. The system works by first \n",
    "        retrieving relevant documents from a knowledge base, then using these documents \n",
    "        as context for generating accurate and informed responses.\n",
    "        \n",
    "        The key components of a RAG system include:\n",
    "        1. Document loader and processor\n",
    "        2. Text chunking strategy\n",
    "        3. Embedding model for vectorization\n",
    "        4. Vector store for efficient retrieval\n",
    "        5. Retrieval strategy\n",
    "        6. Language model for generation\n",
    "        7. Evaluation metrics\n",
    "        \"\"\")\n",
    "    DOCUMENT_PATH = \"./documents/sample.txt\"\n",
    "\n",
    "# Load and process documents\n",
    "chunks = rag_system.load_and_process_documents(DOCUMENT_PATH)\n",
    "\n",
    "# Create or load vector store\n",
    "# force_rebuild=True will recreate the vector store even if it exists\n",
    "vector_store = rag_system.create_or_load_vector_store(chunks, force_rebuild=True)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Processing Summary:\")\n",
    "print(f\"- Chunks created: {len(chunks)}\")\n",
    "print(f\"- Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} chars\")\n",
    "\n",
    "# Test the vector store with a sample query\n",
    "rag_system.vs_manager.test_retrieval(\"What is RAG?\", k=3)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. BUILDING RAG PIPELINES\n",
    "# ============================================================================\n",
    "\n",
    "# Build a linear pipeline (standard RAG flow)\n",
    "pipeline = rag_system.build_pipeline(pipeline_type=\"linear\")\n",
    "\n",
    "# Alternative: Build a parallel pipeline with multiple retrievers\n",
    "# This uses both vector and BM25 retrieval in parallel\n",
    "# pipeline = rag_system.build_pipeline(pipeline_type=\"parallel\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. BASIC QUERYING WITH EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "async def run_simple_query():\n",
    "    \"\"\"Demonstrate simple query without evaluation\"\"\"\n",
    "    \n",
    "    question = \"What is the main topic of the document?\"\n",
    "    \n",
    "    # Query without evaluation - just get the answer\n",
    "    result = await rag_system.query(\n",
    "        question=question,\n",
    "        evaluate=False  # No evaluation metrics\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\u2753 Question: {result['question']}\")\n",
    "    print(f\"\ud83d\udca1 Answer: {result['answer']}\")\n",
    "    print(f\"\u23f1\ufe0f  Response time: {result['response_time']:.2f}s\")\n",
    "    print(f\"\ud83d\udcc4 Retrieved {result['num_contexts']} contexts\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the simple query\n",
    "simple_result = await run_simple_query()\n",
    "\n",
    "# ============================================================================\n",
    "# 7. QUERY WITH AUTOMATIC EVALUATION (NO REFERENCE)\n",
    "# ============================================================================\n",
    "\n",
    "async def run_evaluated_query():\n",
    "    \"\"\"Demonstrate query with evaluation metrics but no reference answer\"\"\"\n",
    "    \n",
    "    # Query with automatic evaluation using modular system\n",
    "    result = await rag_system.query(\n",
    "        question=\"What are the key components of the RAG system?\",\n",
    "        evaluate=True,                  # Enable evaluation\n",
    "        use_modular=True,               # Use new modular evaluation system\n",
    "        metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\ud83d\udca1 Answer: {result['answer']}\")\n",
    "    \n",
    "    # Display evaluation results\n",
    "    if 'evaluation' in result:\n",
    "        print(\"\\n\ud83d\udcca Evaluation Results:\")\n",
    "        for metric_name, metric_result in result['evaluation'].items():\n",
    "            if metric_result['error']:\n",
    "                print(f\"\u274c {metric_name}: Error - {metric_result['error']}\")\n",
    "            else:\n",
    "                print(f\"\u2705 {metric_name}: {metric_result['value']:.3f} \"\n",
    "                      f\"(computed in {metric_result['computation_time']:.2f}s)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the evaluated query\n",
    "evaluated_result = await run_evaluated_query()\n",
    "\n",
    "# ============================================================================\n",
    "# 8. EVALUATION WITH REFERENCE ANSWERS\n",
    "# ============================================================================\n",
    "\n",
    "async def run_reference_evaluation():\n",
    "    \"\"\"Demonstrate evaluation with ground truth reference answers\"\"\"\n",
    "    \n",
    "    # Define test cases with reference answers\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is RAG?\",\n",
    "            \"reference\": \"RAG (Retrieval-Augmented Generation) is a technique that combines \"\n",
    "                        \"retrieval systems with language models to generate more accurate \"\n",
    "                        \"and contextual responses.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How does the evaluation system work?\",\n",
    "            \"reference\": \"The evaluation system uses a modular architecture with pluggable \"\n",
    "                        \"metrics that can assess different aspects of RAG performance \"\n",
    "                        \"including faithfulness, relevancy, and efficiency.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What metrics are available?\",\n",
    "            \"reference\": \"The system supports RAGAS metrics (faithfulness, answer relevancy, \"\n",
    "                        \"context precision/recall), custom metrics (response time, token \"\n",
    "                        \"efficiency, semantic similarity), and is extensible for new metrics.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run evaluation on all test cases\n",
    "    results = []\n",
    "    for test_case in test_cases:\n",
    "        print(f\"\\n\u2753 Evaluating: {test_case['question']}\")\n",
    "        \n",
    "        result = await rag_system.query(\n",
    "            question=test_case[\"question\"],\n",
    "            reference=test_case[\"reference\"],  # Provide reference for comparison\n",
    "            evaluate=True,\n",
    "            use_modular=True,\n",
    "            metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \n",
    "                    \"ragas_context_precision\", \"semantic_similarity\"]\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert results to DataFrame for analysis\n",
    "    eval_df = pd.DataFrame([\n",
    "        {\n",
    "            'question': r['question'],\n",
    "            'answer': r['answer'][:100] + '...',  # Truncate for display\n",
    "            **{k: v['value'] for k, v in r.get('evaluation', {}).items() \n",
    "               if not v.get('error')}\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Evaluation Summary:\")\n",
    "    print(eval_df.to_string())\n",
    "    \n",
    "    return results, eval_df\n",
    "\n",
    "# Run reference evaluation\n",
    "ref_results, ref_df = await run_reference_evaluation()\n",
    "\n",
    "# ============================================================================\n",
    "# 9. USING METRIC GROUPS\n",
    "# ============================================================================\n",
    "\n",
    "def demonstrate_metric_groups():\n",
    "    \"\"\"Show how to use predefined metric groups\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udccb Recommended Metric Sets:\")\n",
    "    \n",
    "    # Different use cases have different recommended metrics\n",
    "    use_cases = [\"general\", \"quality_focus\", \"performance_focus\", \"no_reference\"]\n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        metrics = rag_system.metric_factory.get_recommended_metrics(use_case)\n",
    "        print(f\"\\n{use_case}:\")\n",
    "        for metric in metrics:\n",
    "            print(f\"  - {metric}\")\n",
    "    \n",
    "    return use_cases\n",
    "\n",
    "# Show metric groups\n",
    "use_cases = demonstrate_metric_groups()\n",
    "\n",
    "# ============================================================================\n",
    "# 10. A/B TESTING DIFFERENT CONFIGURATIONS\n",
    "# ============================================================================\n",
    "\n",
    "async def run_ab_testing():\n",
    "    \"\"\"Compare different RAG configurations\"\"\"\n",
    "    \n",
    "    # Create configuration variants to test\n",
    "    configs = {\n",
    "        \"small_chunks\": RAGConfig(\n",
    "            experiment_name=\"small_chunks\",\n",
    "            chunker=ChunkerConfig(\n",
    "                method=\"recursive\",\n",
    "                chunk_size=200,      # Smaller chunks\n",
    "                chunk_overlap=20\n",
    "            ),\n",
    "            retrieval=RetrievalConfig(\n",
    "                strategy=\"vector\",\n",
    "                top_k=7              # More chunks to compensate\n",
    "            ),\n",
    "            generation=GenerationConfig(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.0\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"large_chunks\": RAGConfig(\n",
    "            experiment_name=\"large_chunks\",\n",
    "            chunker=ChunkerConfig(\n",
    "                method=\"recursive\",\n",
    "                chunk_size=1000,     # Larger chunks\n",
    "                chunk_overlap=100\n",
    "            ),\n",
    "            retrieval=RetrievalConfig(\n",
    "                strategy=\"vector\",\n",
    "                top_k=3              # Fewer chunks needed\n",
    "            ),\n",
    "            generation=GenerationConfig(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.0\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"creative_mode\": RAGConfig(\n",
    "            experiment_name=\"creative_mode\",\n",
    "            chunker=ChunkerConfig(\n",
    "                method=\"recursive\",\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=50\n",
    "            ),\n",
    "            retrieval=RetrievalConfig(\n",
    "                strategy=\"vector\",\n",
    "                top_k=5\n",
    "            ),\n",
    "            generation=GenerationConfig(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.7      # Higher temperature for creativity\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Test questions for comparison\n",
    "    test_questions = [\n",
    "        \"What is the main purpose of this system?\",\n",
    "        \"How does the modular architecture work?\",\n",
    "        \"What are the benefits of using this approach?\"\n",
    "    ]\n",
    "    \n",
    "    # Run experiments for each configuration\n",
    "    experiment_results = {}\n",
    "    \n",
    "    for config_name, config_variant in configs.items():\n",
    "        print(f\"\\n\ud83e\uddea Testing configuration: {config_name}\")\n",
    "        \n",
    "        # Create new system with variant configuration\n",
    "        variant_manager = ConfigManager()\n",
    "        variant_manager.config = config_variant\n",
    "        variant_system = ModularRAGSystem(variant_manager)\n",
    "        variant_system.initialize_components()\n",
    "        \n",
    "        # Reuse existing vector store to save time\n",
    "        variant_system.vs_manager = rag_system.vs_manager\n",
    "        variant_system.retriever_factory.vector_store = vector_store\n",
    "        variant_system.retriever_factory.documents = chunks\n",
    "        \n",
    "        # Build pipeline with new configuration\n",
    "        variant_system.build_pipeline()\n",
    "        \n",
    "        # Run tests\n",
    "        variant_results = []\n",
    "        for question in test_questions:\n",
    "            result = await variant_system.query(\n",
    "                question=question,\n",
    "                evaluate=True,\n",
    "                metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \"response_time\"]\n",
    "            )\n",
    "            variant_results.append(result)\n",
    "            print(f\"  \u2713 Tested: {question[:50]}...\")\n",
    "        \n",
    "        experiment_results[config_name] = variant_results\n",
    "    \n",
    "    return experiment_results, test_questions\n",
    "\n",
    "# Run A/B testing\n",
    "experiment_results, test_questions = await run_ab_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 11. RESULTS ANALYSIS AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_experiment_results(experiment_results):\n",
    "    \"\"\"Analyze and visualize A/B test results\"\"\"\n",
    "    \n",
    "    # Prepare data for analysis\n",
    "    analysis_data = []\n",
    "    for config_name, results in experiment_results.items():\n",
    "        for result in results:\n",
    "            row = {\n",
    "                'config': config_name,\n",
    "                'question': result['question'],\n",
    "                'response_time': result['response_time'],\n",
    "                'num_contexts': result['num_contexts']\n",
    "            }\n",
    "            \n",
    "            # Add evaluation metrics\n",
    "            if 'evaluation' in result:\n",
    "                for metric_name, metric_data in result['evaluation'].items():\n",
    "                    if not metric_data.get('error'):\n",
    "                        row[metric_name] = metric_data['value']\n",
    "            \n",
    "            analysis_data.append(row)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\n\ud83d\udcca Summary Statistics by Configuration:\")\n",
    "    summary = analysis_df.groupby('config').agg({\n",
    "        'ragas_faithfulness': 'mean',\n",
    "        'ragas_answer_relevancy': 'mean',\n",
    "        'response_time': 'mean',\n",
    "        'num_contexts': 'mean'\n",
    "    }).round(3)\n",
    "    print(summary)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Configuration Comparison', fontsize=16)\n",
    "    \n",
    "    # 1. Faithfulness comparison\n",
    "    sns.boxplot(data=analysis_df, x='config', y='ragas_faithfulness', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Faithfulness Scores')\n",
    "    axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45)\n",
    "    axes[0, 0].set_ylim(0, 1.1)\n",
    "    \n",
    "    # 2. Answer relevancy comparison\n",
    "    sns.boxplot(data=analysis_df, x='config', y='ragas_answer_relevancy', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Answer Relevancy Scores')\n",
    "    axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45)\n",
    "    axes[0, 1].set_ylim(0, 1.1)\n",
    "    \n",
    "    # 3. Response time comparison\n",
    "    sns.boxplot(data=analysis_df, x='config', y='response_time', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Response Time (seconds)')\n",
    "    axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 4. Create a summary comparison\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create a text summary\n",
    "    best_config = summary.idxmax()\n",
    "    worst_config = summary.idxmin()\n",
    "    \n",
    "    summary_text = \"Performance Summary:\\n\\n\"\n",
    "    summary_text += \"Best performers:\\n\"\n",
    "    for metric, config in best_config.items():\n",
    "        summary_text += f\"- {metric}: {config} ({summary.loc[config, metric]:.3f})\\n\"\n",
    "    \n",
    "    summary_text += \"\\nAreas for improvement:\\n\"\n",
    "    for metric, config in worst_config.items():\n",
    "        if metric != 'response_time':  # Lower is better for response time\n",
    "            summary_text += f\"- {metric}: {config} ({summary.loc[config, metric]:.3f})\\n\"\n",
    "    \n",
    "    ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, \n",
    "            fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analysis_df, summary\n",
    "\n",
    "# Analyze the experiment results\n",
    "analysis_df, summary = analyze_experiment_results(experiment_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0562dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 12. CREATING CUSTOM METRICS\n",
    "# ============================================================================\n",
    "\n",
    "class CompletenessMetric(BaseMetric):\n",
    "    \"\"\"\n",
    "    Custom metric to evaluate answer completeness.\n",
    "    \n",
    "    This metric checks:\n",
    "    1. Whether the answer addresses key question words\n",
    "    2. Whether the answer length is reasonable\n",
    "    3. Overall coverage of the question topic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"answer_completeness\",\n",
    "            requirements=MetricRequirements(\n",
    "                requires_reference=False,    # Doesn't need ground truth\n",
    "                requires_contexts=True,      # Uses retrieved contexts\n",
    "                requires_question=True,      # Needs the question\n",
    "                requires_answer=True         # Needs the generated answer\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    async def compute(self, **kwargs) -> MetricResult:\n",
    "        \"\"\"Compute the completeness score\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract inputs\n",
    "        question = kwargs['question']\n",
    "        answer = kwargs['answer']\n",
    "        contexts = kwargs['contexts']\n",
    "        \n",
    "        # Simple heuristic: check if answer addresses key question words\n",
    "        question_words = set(question.lower().split())\n",
    "        answer_words = set(answer.lower().split())\n",
    "        \n",
    "        # Remove common words that don't carry meaning\n",
    "        common_words = {\n",
    "            'the', 'a', 'an', 'is', 'are', 'was', 'were', \n",
    "            'what', 'how', 'why', 'when', 'where', 'who',\n",
    "            'of', 'to', 'for', 'with', 'in', 'on', 'at'\n",
    "        }\n",
    "        question_words -= common_words\n",
    "        \n",
    "        # Calculate coverage of question words in answer\n",
    "        covered_words = question_words.intersection(answer_words)\n",
    "        coverage = len(covered_words) / len(question_words) if question_words else 0\n",
    "        \n",
    "        # Check if answer length is reasonable (normalize to 50 words)\n",
    "        word_count = len(answer.split())\n",
    "        length_score = min(1.0, word_count / 50)\n",
    "        \n",
    "        # Check if answer uses information from contexts\n",
    "        context_text = ' '.join(contexts).lower()\n",
    "        context_words = set(context_text.split()) - common_words\n",
    "        answer_context_overlap = answer_words.intersection(context_words)\n",
    "        context_usage = len(answer_context_overlap) / len(answer_words) if answer_words else 0\n",
    "        \n",
    "        # Combined score with weights\n",
    "        completeness_score = (\n",
    "            coverage * 0.4 +        # 40% weight on addressing question\n",
    "            length_score * 0.3 +    # 30% weight on answer length\n",
    "            context_usage * 0.3     # 30% weight on using context\n",
    "        )\n",
    "        \n",
    "        return MetricResult(\n",
    "            metric_name=self.name,\n",
    "            value=completeness_score,\n",
    "            metadata={\n",
    "                \"question_coverage\": coverage,\n",
    "                \"length_score\": length_score,\n",
    "                \"context_usage\": context_usage,\n",
    "                \"answer_word_count\": word_count,\n",
    "                \"covered_keywords\": list(covered_words)[:5]  # Top 5 covered words\n",
    "            },\n",
    "            computation_time=time.time() - start_time\n",
    "        )\n",
    "\n",
    "# Register the custom metric\n",
    "completeness_metric = CompletenessMetric()\n",
    "rag_system.metric_factory.registry.register_metric(\n",
    "    completeness_metric,\n",
    "    groups=[\"custom\", \"quality\"]  # Add to custom and quality groups\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Registered custom metric: {completeness_metric.name}\")\n",
    "\n",
    "# Test the custom metric\n",
    "async def test_custom_metric():\n",
    "    \"\"\"Test our custom completeness metric\"\"\"\n",
    "    \n",
    "    result = await rag_system.query(\n",
    "        question=\"What are all the components of the RAG pipeline and how do they work together?\",\n",
    "        evaluate=True,\n",
    "        metrics=[\"answer_completeness\", \"ragas_faithfulness\", \"response_time\"]\n",
    "    )\n",
    "    \n",
    "    # Display custom metric results\n",
    "    if 'evaluation' in result and 'answer_completeness' in result['evaluation']:\n",
    "        completeness_data = result['evaluation']['answer_completeness']\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Answer Completeness Analysis:\")\n",
    "        print(f\"Overall Score: {completeness_data['value']:.3f}\")\n",
    "        print(\"\\nDetailed Breakdown:\")\n",
    "        for key, value in completeness_data['metadata'].items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run custom metric test\n",
    "custom_result = await test_custom_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 13. BATCH EVALUATION AND EXPORT\n",
    "# ============================================================================\n",
    "\n",
    "async def run_batch_evaluation():\n",
    "    \"\"\"Run evaluation on multiple questions and export results\"\"\"\n",
    "    \n",
    "    # Prepare a comprehensive test suite\n",
    "    test_suite = [\n",
    "        {\"question\": \"What is the purpose of the metrics registry?\", \"category\": \"architecture\"},\n",
    "        {\"question\": \"How do I add a new evaluation metric?\", \"category\": \"usage\"},\n",
    "        {\"question\": \"What are the different pipeline types available?\", \"category\": \"features\"},\n",
    "        {\"question\": \"How does the vector store work?\", \"category\": \"technical\"},\n",
    "        {\"question\": \"What configuration options are available?\", \"category\": \"configuration\"},\n",
    "        {\"question\": \"How can I optimize retrieval performance?\", \"category\": \"optimization\"},\n",
    "        {\"question\": \"What are the benefits of modular design?\", \"category\": \"architecture\"},\n",
    "        {\"question\": \"How do I export evaluation results?\", \"category\": \"usage\"}\n",
    "    ]\n",
    "    \n",
    "    # Run batch evaluation\n",
    "    batch_results = []\n",
    "    print(\"\\n\ud83d\udd04 Running batch evaluation...\")\n",
    "    \n",
    "    for i, test in enumerate(test_suite):\n",
    "        print(f\"\\n[{i+1}/{len(test_suite)}] Evaluating: {test['question'][:50]}...\")\n",
    "        \n",
    "        result = await rag_system.query(\n",
    "            question=test[\"question\"],\n",
    "            evaluate=True,\n",
    "            metrics=[\"ragas_faithfulness\", \"ragas_answer_relevancy\", \n",
    "                    \"answer_completeness\", \"response_time\"]\n",
    "        )\n",
    "        \n",
    "        # Add category to result\n",
    "        result['category'] = test['category']\n",
    "        batch_results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    batch_df = pd.DataFrame([\n",
    "        {\n",
    "            'question': r['question'],\n",
    "            'category': r['category'],\n",
    "            'answer_preview': r['answer'][:100] + '...',\n",
    "            **{k: v['value'] for k, v in r.get('evaluation', {}).items() \n",
    "               if not v.get('error')}\n",
    "        }\n",
    "        for r in batch_results\n",
    "    ])\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\ud83d\udcca Batch Evaluation Results:\")\n",
    "    print(batch_df.to_string())\n",
    "    \n",
    "    # Export results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"./results/rag_evaluation_{timestamp}.csv\"\n",
    "    batch_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n\ud83d\udcc1 Results exported to: {output_file}\")\n",
    "    \n",
    "    # Also save detailed results as JSON\n",
    "    import json\n",
    "    json_file = f\"./results/rag_evaluation_detailed_{timestamp}.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(batch_results, f, indent=2, default=str)\n",
    "    print(f\"\ud83d\udcc1 Detailed results saved to: {json_file}\")\n",
    "    \n",
    "    return batch_df, batch_results\n",
    "\n",
    "# Run batch evaluation\n",
    "batch_df, batch_results = await run_batch_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c21a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(batch_df):\n",
    "    \"\"\"Perform statistical analysis on evaluation results\"\"\"\n",
    "    \n",
    "    if len(batch_df) == 0:\n",
    "        print(\"No data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc8 Statistical Analysis\")\n",
    "    \n",
    "    # 1. Summary by category\n",
    "    print(\"\\n1. Performance by Category:\")\n",
    "    category_summary = batch_df.groupby('category').agg({\n",
    "        'ragas_faithfulness': ['mean', 'std', 'min', 'max'],\n",
    "        'ragas_answer_relevancy': ['mean', 'std', 'min', 'max'],\n",
    "        'answer_completeness': ['mean', 'std', 'min', 'max'],\n",
    "        'response_time': ['mean', 'std', 'min', 'max']\n",
    "    }).round(3)\n",
    "    print(category_summary)\n",
    "    \n",
    "    # 2. Correlation analysis\n",
    "    print(\"\\n2. Metric Correlations:\")\n",
    "    numeric_cols = ['ragas_faithfulness', 'ragas_answer_relevancy', \n",
    "                   'answer_completeness', 'response_time']\n",
    "    correlation_matrix = batch_df[numeric_cols].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', \n",
    "                center=0, vmin=-1, vmax=1)\n",
    "    plt.title('Metric Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Identify insights\n",
    "    print(\"\\n3. Key Insights:\")\n",
    "    \n",
    "    # Best performing category\n",
    "    best_category = category_summary['ragas_faithfulness']['mean'].idxmax()\n",
    "    print(f\"- Best performing category: {best_category}\")\n",
    "    \n",
    "    # Most consistent category (lowest std deviation)\n",
    "    most_consistent = category_summary['ragas_faithfulness']['std'].idxmin()\n",
    "    print(f\"- Most consistent category: {most_consistent}\")\n",
    "    \n",
    "    # Correlation insights\n",
    "    strong_correlations = []\n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.5:  # Strong correlation threshold\n",
    "                strong_correlations.append(\n",
    "                    f\"{numeric_cols[i]} \u2194 {numeric_cols[j]}: {corr_value:.3f}\"\n",
    "                )\n",
    "    \n",
    "    if strong_correlations:\n",
    "        print(\"\\n- Strong correlations found:\")\n",
    "        for corr in strong_correlations:\n",
    "            print(f\"  \u2022 {corr}\")\n",
    "    \n",
    "    # 4. Performance distribution\n",
    "    print(\"\\n4. Overall Performance Distribution:\")\n",
    "    performance_stats = batch_df[numeric_cols].describe()\n",
    "    print(performance_stats)\n",
    "    \n",
    "    return category_summary, correlation_matrix\n",
    "\n",
    "# Perform statistical analysis\n",
    "category_summary, correlation_matrix = perform_statistical_analysis(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 15. SAVE CONFIGURATION AND FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "# Save successful configuration for future use\n",
    "config_path = \"./configs/successful_demo_config.yaml\"\n",
    "config_manager.save_config(config_path)\n",
    "print(f\"\\n\u2705 Configuration saved to: {config_path}\")\n",
    "\n",
    "# Create a final summary report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83c\udf89 RAG EVALUATION PIPELINE - COMPLETE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\ud83d\udccb What we accomplished:\")\n",
    "print(\"\u2713 Configured and initialized the modular RAG system\")\n",
    "print(\"\u2713 Loaded and processed documents into vector store\")\n",
    "print(\"\u2713 Built and tested different pipeline types\")\n",
    "print(\"\u2713 Used the flexible modular evaluation system\")\n",
    "print(\"\u2713 Ran A/B tests comparing different configurations\")\n",
    "print(\"\u2713 Created and registered custom metrics\")\n",
    "print(\"\u2713 Performed batch evaluation on multiple questions\")\n",
    "print(\"\u2713 Analyzed results with statistical methods\")\n",
    "print(\"\u2713 Exported results for further analysis\")\n",
    "\n",
    "print(\"\\n\ud83c\udfc6 Best Practices Learned:\")\n",
    "print(\"1. Configuration Management:\")\n",
    "print(\"   - Use meaningful experiment names and tags\")\n",
    "print(\"   - Save successful configurations for reuse\")\n",
    "print(\"   - Document configuration choices\")\n",
    "\n",
    "print(\"\\n2. Metric Selection:\")\n",
    "print(\"   - Choose metrics based on your specific use case\")\n",
    "print(\"   - Use reference-based metrics when ground truth is available\")\n",
    "print(\"   - Combine quality and performance metrics for balance\")\n",
    "\n",
    "print(\"\\n3. Performance Optimization:\")\n",
    "print(\"   - Reuse vector stores when possible\")\n",
    "print(\"   - Batch queries for efficiency\")\n",
    "print(\"   - Monitor token usage and costs\")\n",
    "\n",
    "print(\"\\n4. Custom Metrics:\")\n",
    "print(\"   - Create domain-specific metrics for your needs\")\n",
    "print(\"   - Validate metrics with known test cases\")\n",
    "print(\"   - Register metrics with appropriate groups\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Next Steps:\")\n",
    "print(\"1. Customize for your domain:\")\n",
    "print(\"   - Add domain-specific evaluation metrics\")\n",
    "print(\"   - Create custom chunking strategies\")\n",
    "print(\"   - Implement specialized retrieval methods\")\n",
    "\n",
    "print(\"\\n2. Scale up experiments:\")\n",
    "print(\"   - Use experiment runner for large-scale grid searches\")\n",
    "print(\"   - Implement parallel processing\")\n",
    "print(\"   - Set up automated pipelines\")\n",
    "\n",
    "print(\"\\n3. Enhance evaluation:\")\n",
    "print(\"   - Integrate additional evaluation frameworks\")\n",
    "print(\"   - Add human evaluation interfaces\")\n",
    "print(\"   - Implement online evaluation with user feedback\")\n",
    "\n",
    "print(\"\\n4. Production deployment:\")\n",
    "print(\"   - Add caching for improved performance\")\n",
    "print(\"   - Implement monitoring and logging\")\n",
    "print(\"   - Create API endpoints for the RAG system\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Thank you for using the RAG Evaluation Pipeline!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}