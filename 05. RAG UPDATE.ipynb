{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b5bfa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Import our modules\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     RAGConfig, MetricsConfig, \n\u001b[1;32m     20\u001b[0m     LoaderConfig, ChunkerConfig, EmbeddingConfig,\n\u001b[1;32m     21\u001b[0m     RetrievalConfig, GenerationConfig\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msystem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModularRAGSystem\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msystem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExperimentRunner\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresults\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResultsAnalyzer\n",
      "File \u001b[0;32m~/Documents/LSE/Courses/Capstone /Group Repos/RAG Pipeline/system/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# =====================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# system/__init__.py\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mMain RAG system orchestrator module.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModularRAGSystem\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExperimentRunner\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModularRAGSystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperimentRunner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/LSE/Courses/Capstone /Group Repos/RAG Pipeline/system/rag_system.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPipelineBuilder\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragas_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGASEvaluator, RAGAS_AVAILABLE\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomEvaluator\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morchestrator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationOrchestrator\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodular_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModularEvaluator\n",
      "File \u001b[0;32m~/Documents/LSE/Courses/Capstone /Group Repos/RAG Pipeline/evaluation/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mEvaluation framework modules.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragas_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGASEvaluator\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcofe_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoFERAGEvaluator\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01momnieval_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OmniEvalEvaluator\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morchestrator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationOrchestrator\n",
      "File \u001b[0;32m~/Documents/LSE/Courses/Capstone /Group Repos/RAG Pipeline/evaluation/cofe_evaluator.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from core.config import (\n",
    "    RAGConfig, MetricsConfig, \n",
    "    LoaderConfig, ChunkerConfig, EmbeddingConfig,\n",
    "    RetrievalConfig, GenerationConfig\n",
    ")\n",
    "from system.rag_system import ModularRAGSystem\n",
    "from system.experiment_runner import ExperimentRunner\n",
    "from analysis.results import ResultsAnalyzer\n",
    "from utils import create_project_directories, verify_api_keys\n",
    "\n",
    "# Setup directories\n",
    "dirs = create_project_directories()\n",
    "print(\"‚úì Project directories created\")\n",
    "\n",
    "# Verify API keys\n",
    "api_status = verify_api_keys()\n",
    "if not api_status.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  Please set your OPENAI_API_KEY environment variable\")\n",
    "else:\n",
    "    print(\"‚úì API keys verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0577d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your test questions from the spreadsheet\n",
    "# Update this path to match your actual file location\n",
    "questions_file = \"./data/ab_testing_questions.xlsx\"  # Update this path\n",
    "\n",
    "# Read the Excel file\n",
    "try:\n",
    "    questions_df = pd.read_excel(questions_file)\n",
    "    print(f\"‚úì Loaded {len(questions_df)} questions from spreadsheet\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è  Could not find {questions_file}\")\n",
    "    print(\"Please update the path to your Excel file\")\n",
    "    # For demonstration, let's parse a few from your provided data\n",
    "    questions_data = [\n",
    "        {\n",
    "            \"Category\": \"Data Preparation\",\n",
    "            \"Example Question\": \"What should I do if my file size exceeds the 4GB limit?\",\n",
    "            \"Solution Type\": \"Diagnostic\",\n",
    "            \"Follow Up Question\": \"How should I sample new data without biasing results?\",\n",
    "            \"Follow Up Solution Type\": \"Procedural\"\n",
    "        },\n",
    "        {\n",
    "            \"Category\": \"Data Preparation\", \n",
    "            \"Example Question\": \"What is the variant assignment column, and why is it important?\",\n",
    "            \"Solution Type\": \"Clarifying\",\n",
    "            \"Follow Up Question\": \"Some variant assignments are null; how does this impact my current experiment?\",\n",
    "            \"Follow Up Solution Type\": \"Diagnostic\"\n",
    "        },\n",
    "        {\n",
    "            \"Category\": \"Experimental Design\",\n",
    "            \"Example Question\": \"What is Type I error (Œ±), and how does it affect my experiment?\",\n",
    "            \"Solution Type\": \"Clarifying\",\n",
    "            \"Follow Up Question\": \"What effect does my chosen alpha (0.05) have on false positives here?\",\n",
    "            \"Follow Up Solution Type\": \"Clarifying\"\n",
    "        },\n",
    "        {\n",
    "            \"Category\": \"Assumption Checking\",\n",
    "            \"Example Question\": \"If the QQ plot for an outcome metric deviates significantly from the diagonal line, what does this imply about the data distribution?\",\n",
    "            \"Solution Type\": \"Diagnostic\",\n",
    "            \"Follow Up Question\": \"What if Q-Q plots deviate from diagonal in my revenue data?\",\n",
    "            \"Follow Up Solution Type\": \"Diagnostic\"\n",
    "        },\n",
    "        {\n",
    "            \"Category\": \"CUPED Adjustment\",\n",
    "            \"Example Question\": \"What is CUPED and how does it improve my experiment analysis?\",\n",
    "            \"Solution Type\": \"Clarifying\",\n",
    "            \"Follow Up Question\": \"How can I validate that CUPED adjustment improved sensitivity in my current dataset?\",\n",
    "            \"Follow Up Solution Type\": \"Diagnostic\"\n",
    "        }\n",
    "    ]\n",
    "    questions_df = pd.DataFrame(questions_data)\n",
    "\n",
    "# Display question statistics\n",
    "print(\"\\nQuestion Distribution:\")\n",
    "print(questions_df['Category'].value_counts())\n",
    "print(\"\\nSolution Type Distribution:\")\n",
    "print(questions_df['Solution Type'].value_counts())\n",
    "\n",
    "# Prepare questions for testing\n",
    "test_questions = []\n",
    "for idx, row in questions_df.iterrows():\n",
    "    # Main question\n",
    "    test_questions.append({\n",
    "        \"id\": f\"q_{idx}_main\",\n",
    "        \"question\": row['Example Question'],\n",
    "        \"category\": row['Category'].lower().replace(' ', '_'),\n",
    "        \"solution_type\": row['Solution Type'].lower(),\n",
    "        \"is_follow_up\": False\n",
    "    })\n",
    "    \n",
    "    # Follow-up question if exists\n",
    "    if pd.notna(row.get('Follow Up Question')):\n",
    "        test_questions.append({\n",
    "            \"id\": f\"q_{idx}_followup\",\n",
    "            \"question\": row['Follow Up Question'],\n",
    "            \"category\": row['Category'].lower().replace(' ', '_'),\n",
    "            \"solution_type\": row.get('Follow Up Solution Type', 'clarifying').lower(),\n",
    "            \"is_follow_up\": True\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úì Prepared {len(test_questions)} test questions (including follow-ups)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fe899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your knowledge base PDF\n",
    "# Update this to match your actual PDF location\n",
    "knowledge_base_path = \"./documents\"  # Update if your PDF is elsewhere\n",
    "pdf_file = \"ab_testing_knowledge_base.pdf\"  # Update with your actual PDF filename\n",
    "\n",
    "# Check if PDF exists\n",
    "pdf_path = Path(knowledge_base_path) / pdf_file\n",
    "if not pdf_path.exists():\n",
    "    print(f\"‚ö†Ô∏è  PDF not found at {pdf_path}\")\n",
    "    print(\"Please update the path to your knowledge base PDF\")\n",
    "    print(\"\\nLooking for PDFs in documents folder...\")\n",
    "    pdf_files = list(Path(knowledge_base_path).glob(\"*.pdf\"))\n",
    "    if pdf_files:\n",
    "        print(\"Found PDFs:\")\n",
    "        for i, pdf in enumerate(pdf_files):\n",
    "            print(f\"  {i+1}. {pdf.name}\")\n",
    "        # Use the first PDF found\n",
    "        pdf_path = pdf_files[0]\n",
    "        print(f\"\\nUsing: {pdf_path}\")\n",
    "else:\n",
    "    print(f\"‚úì Found knowledge base PDF: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration optimized for A/B testing Q&A\n",
    "base_config = RAGConfig(\n",
    "    experiment_name=\"ab_testing_qa_evaluation\",\n",
    "    experiment_id=datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \n",
    "    # Document processing - for PDF\n",
    "    loader=LoaderConfig(type=\"pdf\"),\n",
    "    \n",
    "    # Chunking strategy optimized for technical Q&A\n",
    "    chunker=ChunkerConfig(\n",
    "        method=\"recursive\",\n",
    "        chunk_size=400,  # Smaller chunks for precise answers\n",
    "        chunk_overlap=100  # Good overlap for context\n",
    "    ),\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding=EmbeddingConfig(\n",
    "        provider=\"openai\",\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ),\n",
    "    \n",
    "    # Retrieval - hybrid for best results\n",
    "    retrieval=RetrievalConfig(\n",
    "        strategy=\"hybrid\",\n",
    "        top_k=7  # More contexts for comprehensive answers\n",
    "    ),\n",
    "    \n",
    "    # Generation\n",
    "    generation=GenerationConfig(\n",
    "        provider=\"openai\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.1,  # Low for consistency\n",
    "        max_tokens=800\n",
    "    ),\n",
    "    \n",
    "    # Comprehensive evaluation metrics\n",
    "    metrics=MetricsConfig(\n",
    "        metric_names=[\n",
    "            # RAGAS metrics\n",
    "            \"ragas_faithfulness\",\n",
    "            \"ragas_answer_relevancy\",\n",
    "            # CoFE-RAG metrics\n",
    "            \"cofe_retrieval_recall\",\n",
    "            \"cofe_retrieval_accuracy\", \n",
    "            \"cofe_generation_faithfulness\",\n",
    "            \"cofe_generation_relevance\",\n",
    "            \"cofe_pipeline_score\",\n",
    "            # OmniEval metrics\n",
    "            \"omni_accuracy\",\n",
    "            \"omni_completeness\",\n",
    "            \"omni_hallucination\",\n",
    "            \"omni_utilization\",\n",
    "            \"omni_weighted_score\",\n",
    "            # General\n",
    "            \"response_time\",\n",
    "            \"aggregate_score\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize system\n",
    "config_manager = ConfigManager(base_config)\n",
    "rag_system = ModularRAGSystem(config_manager)\n",
    "rag_system.initialize_components()\n",
    "print(\"‚úì RAG system initialized\")\n",
    "\n",
    "# Load and process the PDF\n",
    "chunks = rag_system.load_and_process_documents(str(pdf_path.parent))\n",
    "print(f\"‚úì Processed {len(chunks)} chunks from PDF\")\n",
    "\n",
    "# Show sample chunks to verify loading\n",
    "print(\"\\nSample chunks from your knowledge base:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Content: {chunk.page_content[:150]}...\")\n",
    "    print(f\"Source: {chunk.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Page: {chunk.metadata.get('page', 'N/A')}\")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = rag_system.create_or_load_vector_store(chunks, force_rebuild=True)\n",
    "print(\"\\n‚úì Vector store created\")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = rag_system.build_pipeline()\n",
    "print(\"‚úì Pipeline built - ready for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222944c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions by category\n",
    "categories = list(set(q['category'] for q in test_questions))\n",
    "\n",
    "print(\"Testing Questions by Category:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "category_results = {}\n",
    "\n",
    "for category in categories[:5]:  # Test first 5 categories\n",
    "    print(f\"\\n\\nCATEGORY: {category.upper().replace('_', ' ')}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Get questions for this category\n",
    "    cat_questions = [q for q in test_questions if q['category'] == category][:2]  # Max 2 per category\n",
    "    \n",
    "    category_scores = []\n",
    "    \n",
    "    for q in cat_questions:\n",
    "        print(f\"\\nQuestion: {q['question']}\")\n",
    "        print(f\"Type: {q['solution_type']} | Follow-up: {q['is_follow_up']}\")\n",
    "        \n",
    "        # Run query\n",
    "        result = await rag_system.query(\n",
    "            question=q['question'],\n",
    "            evaluate=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nAnswer:\\n{result['answer'][:300]}...\")  # Show first 300 chars\n",
    "        \n",
    "        # Store scores\n",
    "        if 'evaluation' in result:\n",
    "            eval_scores = result['evaluation']\n",
    "            scores = {\n",
    "                'faithfulness': eval_scores.get('ragas_faithfulness', 0),\n",
    "                'relevancy': eval_scores.get('ragas_answer_relevancy', 0),\n",
    "                'pipeline': eval_scores.get('cofe_pipeline_score', 0),\n",
    "                'omni_score': eval_scores.get('omni_weighted_score', 0),\n",
    "                'aggregate': eval_scores.get('aggregate_score', 0)\n",
    "            }\n",
    "            category_scores.append(scores)\n",
    "            \n",
    "            print(f\"\\nScores:\")\n",
    "            print(f\"  Aggregate: {scores['aggregate']:.3f}\")\n",
    "            print(f\"  Faithfulness: {scores['faithfulness']:.3f}\")\n",
    "            print(f\"  Pipeline: {scores['pipeline']:.3f}\")\n",
    "    \n",
    "    # Calculate category average\n",
    "    if category_scores:\n",
    "        avg_scores = {k: np.mean([s[k] for s in category_scores]) for k in category_scores[0].keys()}\n",
    "        category_results[category] = avg_scores\n",
    "        print(f\"\\nCategory Average Scores:\")\n",
    "        for metric, score in avg_scores.items():\n",
    "            print(f\"  {metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by solution type\n",
    "print(\"\\n\\nTesting by Solution Type:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "solution_types = list(set(q['solution_type'] for q in test_questions))\n",
    "solution_results = {}\n",
    "\n",
    "for sol_type in solution_types:\n",
    "    print(f\"\\n\\nSOLUTION TYPE: {sol_type.upper()}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Get sample questions\n",
    "    type_questions = [q for q in test_questions if q['solution_type'] == sol_type][:3]\n",
    "    \n",
    "    type_scores = []\n",
    "    \n",
    "    for q in type_questions:\n",
    "        print(f\"\\nQ: {q['question'][:100]}...\")\n",
    "        \n",
    "        result = await rag_system.query(q['question'], evaluate=True)\n",
    "        \n",
    "        if 'evaluation' in result:\n",
    "            scores = {\n",
    "                'aggregate': result['evaluation'].get('aggregate_score', 0),\n",
    "                'accuracy': result['evaluation'].get('omni_accuracy', 0),\n",
    "                'completeness': result['evaluation'].get('omni_completeness', 0),\n",
    "                'response_time': result['response_time']\n",
    "            }\n",
    "            type_scores.append(scores)\n",
    "            print(f\"  Score: {scores['aggregate']:.3f} | Time: {scores['response_time']:.2f}s\")\n",
    "    \n",
    "    # Average for this type\n",
    "    if type_scores:\n",
    "        avg_scores = {k: np.mean([s[k] for s in type_scores]) for k in type_scores[0].keys()}\n",
    "        solution_results[sol_type] = avg_scores\n",
    "        print(f\"\\nAverage for {sol_type}:\")\n",
    "        print(f\"  Score: {avg_scores['aggregate']:.3f}\")\n",
    "        print(f\"  Response Time: {avg_scores['response_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration variants for A/B testing\n",
    "ab_test_variants = {\n",
    "    # Chunking strategies\n",
    "    \"chunker.method\": [\"recursive\", \"semantic\", \"sentence\"],\n",
    "    \"chunker.chunk_size\": [300, 400, 600],\n",
    "    \n",
    "    # Retrieval strategies\n",
    "    \"retrieval.strategy\": [\"vector\", \"hybrid\", \"bm25\"],\n",
    "    \"retrieval.top_k\": [5, 7, 10],\n",
    "    \n",
    "    # Generation parameters\n",
    "    \"generation.temperature\": [0.0, 0.1, 0.3],\n",
    "    \"generation.model\": [\"gpt-4o-mini\", \"gpt-4\"]  # Remove if you don't have GPT-4 access\n",
    "}\n",
    "\n",
    "# For quick testing, use a subset\n",
    "quick_variants = {\n",
    "    \"chunker.method\": [\"recursive\", \"semantic\"],\n",
    "    \"chunker.chunk_size\": [400, 600],\n",
    "    \"retrieval.strategy\": [\"vector\", \"hybrid\"],\n",
    "    \"retrieval.top_k\": [5, 7]\n",
    "}\n",
    "\n",
    "# Select representative questions for A/B testing\n",
    "ab_test_questions = []\n",
    "for category in categories[:5]:  # Use 5 categories\n",
    "    cat_q = [q for q in test_questions if q['category'] == category][:2]  # 2 questions per category\n",
    "    ab_test_questions.extend(cat_q)\n",
    "\n",
    "print(f\"Running A/B test with:\")\n",
    "print(f\"  - {len(ab_test_questions)} test questions\")\n",
    "print(f\"  - Testing variants: {quick_variants}\")\n",
    "\n",
    "# Initialize experiment runner\n",
    "runner = ExperimentRunner(base_path=\"./experiments/ab_testing_qa\")\n",
    "\n",
    "# Run A/B test\n",
    "results_df = await runner.run_ab_test(\n",
    "    base_config=base_config,\n",
    "    variants=quick_variants,\n",
    "    test_questions=[{\"question\": q[\"question\"], \n",
    "                    \"category\": q[\"category\"],\n",
    "                    \"solution_type\": q[\"solution_type\"]} \n",
    "                   for q in ab_test_questions],\n",
    "    document_path=str(pdf_path.parent)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì A/B test completed!\")\n",
    "print(f\"Total evaluations: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d44586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze A/B test results\n",
    "analyzer = ResultsAnalyzer(results_df)\n",
    "\n",
    "# Get best configuration\n",
    "best_config = analyzer.get_best_configuration()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"A/B TESTING RESULTS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "print(f\"Configuration: {best_config['configuration']}\")\n",
    "print(f\"Average Score: {best_config['mean_score']:.3f} (¬±{best_config['std']:.3f})\")\n",
    "print(f\"\\nDetails:\")\n",
    "for key, value in best_config['details'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Analyze by component\n",
    "print(\"\\n\\nCOMPONENT ANALYSIS:\")\n",
    "print(\"-\"*60)\n",
    "component_analysis = analyzer.analyze_by_component()\n",
    "\n",
    "for component, analysis in component_analysis.items():\n",
    "    print(f\"\\n{component.upper()}:\")\n",
    "    print(analysis.head())\n",
    "    best_value = analysis['mean'].idxmax()\n",
    "    print(f\"  ‚Üí Best: {best_value} (score: {analysis.loc[best_value, 'mean']:.3f})\")\n",
    "\n",
    "# Category-specific performance\n",
    "if 'category' in results_df.columns:\n",
    "    print(\"\\n\\nPERFORMANCE BY CATEGORY:\")\n",
    "    print(\"-\"*60)\n",
    "    cat_performance = results_df.groupby(['category', 'config_description'])['aggregate_score'].mean()\n",
    "    \n",
    "    for category in results_df['category'].unique():\n",
    "        cat_data = cat_performance[category]\n",
    "        best_cat_config = cat_data.idxmax()\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(f\"  Best Config: {best_cat_config}\")\n",
    "        print(f\"  Score: {cat_data.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16191572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "analyzer.create_performance_dashboard()\n",
    "\n",
    "# Additional custom visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Category Performance Heatmap\n",
    "ax1 = axes[0, 0]\n",
    "if 'category' in results_df.columns:\n",
    "    cat_pivot = results_df.pivot_table(\n",
    "        values='aggregate_score',\n",
    "        index='config_description',\n",
    "        columns='category',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    sns.heatmap(cat_pivot, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax1)\n",
    "    ax1.set_title('Performance Heatmap by Category')\n",
    "    ax1.set_xlabel('Category')\n",
    "    ax1.set_ylabel('Configuration')\n",
    "\n",
    "# 2. Solution Type Performance\n",
    "ax2 = axes[0, 1]\n",
    "if 'solution_type' in results_df.columns:\n",
    "    sol_type_scores = results_df.groupby('solution_type')['aggregate_score'].mean().sort_values()\n",
    "    sol_type_scores.plot(kind='barh', ax=ax2, color='skyblue')\n",
    "    ax2.set_title('Average Performance by Solution Type')\n",
    "    ax2.set_xlabel('Aggregate Score')\n",
    "\n",
    "# 3. Response Time Distribution\n",
    "ax3 = axes[1, 0]\n",
    "results_df.boxplot(column='response_time', by='config_description', ax=ax3, rot=90)\n",
    "ax3.set_title('Response Time Distribution by Configuration')\n",
    "ax3.set_ylabel('Response Time (seconds)')\n",
    "\n",
    "# 4. Evaluation Framework Comparison\n",
    "ax4 = axes[1, 1]\n",
    "framework_cols = {\n",
    "    'RAGAS': ['ragas_faithfulness', 'ragas_answer_relevancy'],\n",
    "    'CoFE-RAG': ['cofe_pipeline_score'],\n",
    "    'OmniEval': ['omni_weighted_score']\n",
    "}\n",
    "framework_means = {}\n",
    "for name, cols in framework_cols.items():\n",
    "    available = [c for c in cols if c in results_df.columns]\n",
    "    if available:\n",
    "        framework_means[name] = results_df[available].mean().mean()\n",
    "\n",
    "if framework_means:\n",
    "    pd.Series(framework_means).plot(kind='bar', ax=ax4, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "    ax4.set_title('Average Scores by Evaluation Framework')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./experiments/ab_testing_qa_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing custom questions\n",
    "async def test_ab_question(question: str, show_contexts: bool = False):\n",
    "    \"\"\"Test any A/B testing question and see detailed results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get answer\n",
    "    result = await rag_system.query(question, evaluate=True)\n",
    "    \n",
    "    print(f\"\\nüìù ANSWER:\")\n",
    "    print(\"-\"*40)\n",
    "    print(result['answer'])\n",
    "    \n",
    "    # Show contexts if requested\n",
    "    if show_contexts and 'contexts' in result:\n",
    "        print(f\"\\nüìö RETRIEVED CONTEXTS ({len(result['contexts'])}):\")\n",
    "        print(\"-\"*40)\n",
    "        for i, ctx in enumerate(result['contexts'][:3]):  # Show first 3\n",
    "            print(f\"\\nContext {i+1}:\")\n",
    "            print(ctx[:200] + \"...\")\n",
    "    \n",
    "    # Show evaluation scores\n",
    "    if 'evaluation' in result:\n",
    "        eval_scores = result['evaluation']\n",
    "        print(f\"\\nüìä EVALUATION SCORES:\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"Aggregate Score: {eval_scores.get('aggregate_score', 0):.3f}\")\n",
    "        print(f\"RAGAS Faithfulness: {eval_scores.get('ragas_faithfulness', 0):.3f}\")\n",
    "        print(f\"CoFE Pipeline: {eval_scores.get('cofe_pipeline_score', 0):.3f}\")\n",
    "        print(f\"OmniEval Score: {eval_scores.get('omni_weighted_score', 0):.3f}\")\n",
    "        print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test some real-world A/B testing questions\n",
    "test_scenarios = [\n",
    "    \"My p-value is 0.06 but my boss wants to launch. What should I do?\",\n",
    "    \"How do I know if my sample size is large enough for detecting a 2% uplift?\",\n",
    "    \"The treatment group has 20% fewer users than control. Is my test valid?\",\n",
    "    \"Should I use CUPED if my pre-experiment metric has correlation of 0.25?\",\n",
    "    \"What's the difference between statistical and practical significance?\"\n",
    "]\n",
    "\n",
    "print(\"Testing Real-World A/B Testing Scenarios:\")\n",
    "for scenario in test_scenarios[:3]:  # Test first 3\n",
    "    await test_ab_question(scenario, show_contexts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ca879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_path = f\"./experiments/ab_testing_qa_report_{timestamp}.md\"\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"# A/B Testing Q&A System Evaluation Report\\n\\n\")\n",
    "    f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    f.write(\"## Executive Summary\\n\\n\")\n",
    "    f.write(f\"- **Knowledge Base:** {pdf_path.name}\\n\")\n",
    "    f.write(f\"- **Test Questions:** {len(test_questions)} questions from spreadsheet\\n\")\n",
    "    f.write(f\"- **Configurations Tested:** {results_df['config_description'].nunique()}\\n\")\n",
    "    f.write(f\"- **Best Configuration:** `{best_config['configuration']}`\\n\")\n",
    "    f.write(f\"- **Best Score:** {best_config['mean_score']:.3f}\\n\\n\")\n",
    "    \n",
    "    # Configuration Details\n",
    "    f.write(\"## Optimal Configuration\\n\\n\")\n",
    "    f.write(\"```yaml\\n\")\n",
    "    f.write(f\"chunking:\\n\")\n",
    "    f.write(f\"  method: {best_config['details']['chunker_method']}\\n\")\n",
    "    f.write(f\"  size: {best_config['details']['chunk_size']}\\n\")\n",
    "    f.write(f\"retrieval:\\n\")\n",
    "    f.write(f\"  strategy: {best_config['details']['retriever_strategy']}\\n\")\n",
    "    f.write(f\"  top_k: {best_config['details']['top_k']}\\n\")\n",
    "    f.write(f\"generation:\\n\")\n",
    "    f.write(f\"  model: {best_config['details']['generator_model']}\\n\")\n",
    "    f.write(\"```\\n\\n\")\n",
    "    \n",
    "    # Performance by Category\n",
    "    f.write(\"## Performance by Question Category\\n\\n\")\n",
    "    if category_results:\n",
    "        f.write(\"| Category | Aggregate Score | Faithfulness | Pipeline Score |\\n\")\n",
    "        f.write(\"|----------|----------------|--------------|----------------|\\n\")\n",
    "        for cat, scores in category_results.items():\n",
    "            f.write(f\"| {cat.replace('_', ' ').title()} | {scores['aggregate']:.3f} | \"\n",
    "                   f\"{scores['faithfulness']:.3f} | {scores['pipeline']:.3f} |\\n\")\n",
    "    \n",
    "    # Performance by Solution Type\n",
    "    f.write(\"\\n## Performance by Solution Type\\n\\n\")\n",
    "    if solution_results:\n",
    "        f.write(\"| Solution Type | Avg Score | Avg Response Time |\\n\")\n",
    "        f.write(\"|--------------|-----------|------------------|\\n\")\n",
    "        for sol_type, scores in solution_results.items():\n",
    "            f.write(f\"| {sol_type.title()} | {scores['aggregate']:.3f} | \"\n",
    "                   f\"{scores['response_time']:.2f}s |\\n\")\n",
    "    \n",
    "    # Key Insights\n",
    "    f.write(\"\\n## Key Insights\\n\\n\")\n",
    "    f.write(\"1. **Best Performing Categories:**\\n\")\n",
    "    if category_results:\n",
    "        sorted_cats = sorted(category_results.items(), key=lambda x: x[1]['aggregate'], reverse=True)\n",
    "        for i, (cat, scores) in enumerate(sorted_cats[:3]):\n",
    "            f.write(f\"   - {cat.replace('_', ' ').title()}: {scores['aggregate']:.3f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n2. **Configuration Recommendations:**\\n\")\n",
    "    f.write(f\"   - Chunking: Use `{best_config['details']['chunker_method']}` \")\n",
    "    f.write(f\"with size `{best_config['details']['chunk_size']}`\\n\")\n",
    "    f.write(f\"   - Retrieval: `{best_config['details']['retriever_strategy']}` \")\n",
    "    f.write(f\"with `{best_config['details']['top_k']}` contexts\\n\")\n",
    "    \n",
    "    f.write(\"\\n3. **Performance Characteristics:**\\n\")\n",
    "    f.write(f\"   - Average response time: {results_df['response_time'].mean():.2f}s\\n\")\n",
    "    f.write(f\"   - RAGAS Faithfulness: {results_df['ragas_faithfulness'].mean():.3f}\\n\")\n",
    "    f.write(f\"   - CoFE Pipeline Score: {results_df['cofe_pipeline_score'].mean():.3f}\\n\")\n",
    "    \n",
    "    # Recommendations\n",
    "    f.write(\"\\n## Recommendations\\n\\n\")\n",
    "    f.write(\"1. **Deploy** the optimal configuration for production use\\n\")\n",
    "    f.write(\"2. **Monitor** performance on real user questions\\n\")\n",
    "    f.write(\"3. **Update** the knowledge base PDF regularly with new A/B testing scenarios\\n\")\n",
    "    f.write(\"4. **Collect** user feedback on answer quality and completeness\\n\")\n",
    "    f.write(\"5. **Re-evaluate** monthly with new questions from actual usage\\n\")\n",
    "\n",
    "print(f\"‚úì Report saved to: {report_path}\")\n",
    "\n",
    "# Also export detailed results\n",
    "analyzer.export_results(f\"./experiments/ab_testing_qa_results_{timestamp}.xlsx\")\n",
    "print(f\"‚úì Detailed Excel analysis exported\")\n",
    "\n",
    "# Quick stats summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE - QUICK STATS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Knowledge Base: {pdf_path.name}\")\n",
    "print(f\"Questions Tested: {len(test_questions)}\")\n",
    "print(f\"Best Score: {best_config['mean_score']:.3f}\")\n",
    "print(f\"Avg Response Time: {results_df['response_time'].mean():.2f}s\")\n",
    "print(f\"Best Chunking: {best_config['details']['chunker_method']}\")\n",
    "print(f\"Best Retrieval: {best_config['details']['retriever_strategy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95909777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96e250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
